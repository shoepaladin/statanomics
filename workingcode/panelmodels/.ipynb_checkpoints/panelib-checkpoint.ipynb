{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243a43a9",
   "metadata": {},
   "source": [
    "# panelib\n",
    "\n",
    "Julian Hsu\n",
    "\n",
    "\n",
    "This is our package for panel models. We use our own written synthetic control and a diff-in-diff model using `statsmodels`.\n",
    "\n",
    "**todolist**\n",
    "1. [COMPLETED: 14DEC2022] Correct p-value calculation for SC models \n",
    "2. [COMPLETED: 15DEC2022] Output aggregate ATET and p-value for SC models\n",
    "3. [COMPLETED: 15DEC2022] Write a function to plot SC model output\n",
    "4. Code up DiD Model\n",
    "    1. [COMPLETED: 16DEC2022] OLS model for TWFE\n",
    "    2. [COMPLETED: 16DEC2022] placebo OLS model with event study\n",
    "    3. Output F-test, t-tests, graphical evidence\n",
    "    4. [COMPLETED: 30DEC2022] Output predicted counterfactual.\n",
    "    5. Incorporate covariates\n",
    "5. Incorporate covariates into SC model\n",
    "6. Staggered treatment effect for SC and DiD model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "348a8f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_js_diagrammers extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_js_diagrammers\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from IPython.display import display    \n",
    "\n",
    "%load_ext nb_js_diagrammers\n",
    "\n",
    "import scipy.stats \n",
    "from sklearn.linear_model import ElasticNet\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from typing import List\n",
    "from operator import add\n",
    "from toolz import reduce, partial\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "82b7b4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"&lt;html&gt;\n",
       "    &lt;body&gt;\n",
       "        &lt;script src=&quot;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js&quot;&gt;&lt;/script&gt;\n",
       "        &lt;script&gt;\n",
       "            mermaid.initialize({ startOnLoad: true });\n",
       "        &lt;/script&gt;\n",
       " \n",
       "        &lt;div class=&quot;mermaid&quot;&gt;\n",
       "            \n",
       "flowchart LR\n",
       "    A[Panel Data] --&gt; B[Synthetic Control];\n",
       "    B[Model Choice] --&gt; C[1. Abadie, Diamond and Hainmeuller \\n 2. Doudchenko and Imbens \\n 3. Constrained Lasso ];\n",
       "    B[Inference] --&gt; D[1. Conformal Inference \\n 2. Permutation Test];\n",
       "    C --&gt; E[Placebo test with \\n Test/Training \\n Pre-Experimental Sample];\n",
       "    D --&gt; E;\n",
       "    E --&gt; Z[1. ATET with inference, and \\n 2. Placebo tests that have their own inference];\n",
       "    A --&gt; F[Difference-in-Difference];\n",
       "    F --&gt; G[OLS: TWFE Model];\n",
       "    G --&gt; H[Event-study TWFE model];\n",
       "    H --&gt; Z;\n",
       "    \n",
       "\n",
       "        &lt;/div&gt;\n",
       " \n",
       "    &lt;/body&gt;\n",
       "&lt;/html&gt;\n",
       "\" width=\"100%\" height=\"450\"style=\"border:none !important;\" \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\"></iframe>"
      ],
      "text/plain": [
       "<nb_js_diagrammers.magics.JSDiagram at 0x7f944d20a340>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%mermaid_magic -h 450\n",
    "\n",
    "flowchart LR\n",
    "    A[Panel Data] --> B[Synthetic Control];\n",
    "    B[Model Choice] --> C[1. Abadie, Diamond and Hainmeuller \\n 2. Doudchenko and Imbens \\n 3. Constrained Lasso ];\n",
    "    B[Inference] --> D[1. Conformal Inference \\n 2. Permutation Test];\n",
    "    C --> E[Placebo test with \\n Test/Training \\n Pre-Experimental Sample];\n",
    "    D --> E;\n",
    "    E --> Z[1. ATET with inference, and \\n 2. Placebo tests that have their own inference];\n",
    "    A --> F[Difference-in-Difference];\n",
    "    F --> G[OLS: TWFE Model];\n",
    "    G --> H[Event-study TWFE model];\n",
    "    H --> Z;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "028326b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "class did:\n",
    "    def treated_counterfactual(did_model=None,\n",
    "                            atet=None,\n",
    "                               df = None,\n",
    "                              data=None,\n",
    "                              data_dict={'treatment':None,\n",
    "                                      'date':None,\n",
    "                                      'post':None,\n",
    "                                      'unitid':None,\n",
    "                                      'outcome':None}):\n",
    "        '''\n",
    "        Estimate the counterfactual trend of the units if they were in control.\n",
    "        We do this by using the treated OLS model.\n",
    "        '''\n",
    "        df_c = df.copy()\n",
    "        df_c['y_hats'] = did_model.predict(data)\n",
    "\n",
    "        '''\n",
    "        We simply subtract the estimated ATET from the treated units.\n",
    "        To accommodate ATET that is a constant for each unit, and varies across units, we merge on ATET estimates.\n",
    "        '''\n",
    "        if 'time_period' in atet.columns:\n",
    "#             print(df_c[ [data_dict['unitid'], data_dict['date'] ]].dtypes)\n",
    "#             print(atet[ [ 'treated_unit', 'time_period']].dtypes)\n",
    "            df_c = df_c.merge(atet, left_on=[data_dict['unitid'], data_dict['date']],\n",
    "                              right_on = [ 'treated_unit', 'time_period'], how='left' )\n",
    "            df_c.fillna(0, inplace=True)\n",
    "            df_c['y_hat_counterfactual']   = df_c['y_hats'] -\\\n",
    "                    df_c['coef_']\n",
    "            df_c['y_hat_se'] = df_c['se_']\n",
    "        else:\n",
    "            df_c = df_c.merge(atet, left_on=[data_dict['unitid']],\n",
    "                              right_on=['treated_unit'], how='left' )\n",
    "            df_c.fillna(0, inplace=True)\n",
    "            df_c['y_hat_counterfactual']   = df_c['y_hats'] -\\\n",
    "                        df_c['coef_']*(df_c[data_dict['post']]==1)\n",
    "            df_c['y_hat_se'] = df_c['se_']\n",
    "        return df_c[[data_dict['unitid'], data_dict['date'], \n",
    "                     data_dict['outcome'],\n",
    "                     'y_hats','y_hat_counterfactual','y_hat_se','coef_']]\n",
    "    \n",
    "    def twfe(data=None,\n",
    "             covariates = [],\n",
    "            data_dict={'treatment':None,\n",
    "                      'date':None,\n",
    "                      'post':None,\n",
    "                      'unitid':None,\n",
    "                      'outcome':None}):\n",
    "        ## Construct the TWFE regression by creating time indicators and unit indicators\n",
    "        ## This estimate treatment-unit specific treatment effects\n",
    "        t_fe = pd.get_dummies(data[data_dict['date']]  , drop_first=True, dtype=float)\n",
    "        x_fe = pd.get_dummies(data[data_dict['unitid']], drop_first=True, dtype=float)\n",
    "\n",
    "        treated_units = data.loc[data[data_dict['treatment']]==1][data_dict['unitid']].unique().tolist()\n",
    "\n",
    "        for i,r in zip(range(len(treated_units)),  treated_units):\n",
    "            if i==0:\n",
    "                post_treated = pd.DataFrame(\n",
    "                                           data={'post_x_{0}'.format(r): \n",
    "                                                (data[data_dict['post']]*(data[data_dict['unitid']]==r)).astype(float)})\n",
    "            else:\n",
    "                post_treated['post_x_{0}'.format(r)]=(data[data_dict['post']]*(data[data_dict['unitid']]==r)).astype(float)\n",
    "        if len(covariates)==0:\n",
    "            twfe_X = sm.add_constant( pd.concat([post_treated,  t_fe, x_fe ], axis=1)  )\n",
    "        else:\n",
    "            twfe_X = sm.add_constant( pd.concat([post_treated,  t_fe, x_fe,\\\n",
    "                                                data[covariates]], axis=1)  )\n",
    "        twfe_model = sm.OLS(data[data_dict['outcome']],  twfe_X).fit()\n",
    "        twfe_coef = twfe_model.params.iloc[1:1+len(treated_units)]\n",
    "        twfe_se = twfe_model.bse.iloc[1:1+len(treated_units)]\n",
    "        twfe_tstat = twfe_model.tvalues.iloc[1:1+len(treated_units)]\n",
    "        twfe_pvalues = twfe_model.pvalues.iloc[1:1+len(treated_units)]        \n",
    "        '''\n",
    "        Output a dataframe that tells us the ATET by unit id and (if applicable) time period\n",
    "        '''\n",
    "\n",
    "        \n",
    "        df_twfe = pd.DataFrame()\n",
    "        for r, coef_, se_, pv_ in zip(twfe_coef.index, twfe_coef, twfe_se, twfe_pvalues):\n",
    "            unitid = r.split('_')[-1]\n",
    "            if 'post' in r:\n",
    "                df_twfe = pd.concat([df_twfe,\n",
    "                                         pd.DataFrame(index=[r],\n",
    "                                                     data={'treated_unit':unitid,\n",
    "                                                          'coef_':coef_,\n",
    "                                                          'se_':se_,\n",
    "                                                          'pvalue':pv_}  )])   \n",
    "            else:\n",
    "                pass\n",
    "        df_c = did.treated_counterfactual(did_model=twfe_model,\n",
    "                                        atet=df_twfe,\n",
    "                                        df = data,\n",
    "                                        data = twfe_X,\n",
    "                                        data_dict = data_dict)\n",
    "\n",
    "        '''\n",
    "        Also construct the event-study approach\n",
    "        '''\n",
    "\n",
    "        ## Use the time period right before treatment as the hold out set\n",
    "        treated_units = data.loc[data[data_dict['treatment']]==1][data_dict['unitid']].unique().tolist()\n",
    "        hold_out_time = data.loc[ (data[data_dict['unitid']].isin(treated_units)) &\n",
    "                                  (data[data_dict['post']]==0)][data_dict['date']].max()\n",
    "        event_dummies = pd.DataFrame()\n",
    "        t_period_list = data.sort_values(by=data_dict['date'], ascending=True)[data_dict['date']].unique().tolist()\n",
    "        pre_treat_columns = []\n",
    "        pst_treat_columns = []\n",
    "        for t_period,i in zip(t_period_list, range(len(t_period_list))):\n",
    "            if t_period < hold_out_time:\n",
    "                for t_units_ in treated_units:\n",
    "                    event_dummies = pd.concat([event_dummies,\n",
    "                                              pd.DataFrame(data={\n",
    "                                                  'pre_treat_{0}_{1}'.format(i,t_units_):\n",
    "                                                  ( (data[data_dict['unitid']]==t_units_) *\n",
    "                                                  (data[data_dict['date']]==t_period)).astype(float)                                          \n",
    "                                              })],axis=1)\n",
    "                    pre_treat_columns.append('pre_treat_{0}_{1}'.format(i,t_units_))\n",
    "            elif t_period == hold_out_time:\n",
    "                pass\n",
    "            else:\n",
    "                for t_units_ in treated_units:\n",
    "                    event_dummies = pd.concat([event_dummies,\n",
    "                                              pd.DataFrame(data={\n",
    "                                                  'pst_treat_{0}_{1}'.format(i, t_units_):\n",
    "                                                  ( (data[data_dict['unitid']]==t_units_ )*\n",
    "                                                  (data[data_dict['date']]==t_period)).astype(float)                                          \n",
    "                                              })],axis=1)\n",
    "                    pst_treat_columns.append('pst_treat_{0}_{1}'.format(i,t_units_))\n",
    "\n",
    "        if len(covariates)==0:\n",
    "            event_X = event_X = sm.add_constant( event_dummies  )\n",
    "        else:\n",
    "            event_X = event_X = sm.add_constant( pd.concat([ event_dummies  ,\\\n",
    "                                                data[covariates]], axis=1)  )                                        \n",
    "        event_model = sm.OLS(data[data_dict['outcome']],  event_X).fit()\n",
    "        event_pre_coef = event_model.params[pre_treat_columns]\n",
    "        event_pre_se = event_model.bse[pre_treat_columns]\n",
    "        event_pre_tstat = event_model.tvalues[pre_treat_columns]\n",
    "        event_pre_pvalues = event_model.pvalues[pre_treat_columns]\n",
    "        \n",
    "        event_pst_coef = event_model.params[pst_treat_columns]\n",
    "        event_pst_se = event_model.bse[pst_treat_columns]\n",
    "        event_pst_tstat = event_model.tvalues[pst_treat_columns]\n",
    "        event_pst_pvalues = event_model.pvalues[pst_treat_columns]\n",
    "\n",
    "        \n",
    "\n",
    "        ## Test whether all pre-trend estimates are different from zero\n",
    "        ## Remember to only use the parameters of the post\n",
    "        A = np.identity(len(event_model.params))\n",
    "        remove_row=[]\n",
    "        for i,name in zip(range(len(event_model.params)), event_model.params.index):\n",
    "            if 'pre' in name:\n",
    "                remove_row.append(i)\n",
    "            else:\n",
    "                pass\n",
    "        A = A[remove_row]\n",
    "        A = A[1:,:]       \n",
    "        \n",
    "        try:\n",
    "            FJointStat = event_model.f_test(A).statistic\n",
    "        except:\n",
    "            FJointStat = event_model.f_test(A).statistic.item()\n",
    "        try:\n",
    "            FJointPValue = event_model.f_test(A).pvalue\n",
    "        except:\n",
    "            FJointPValue = event_model.f_test(A).pvalue.item()            \n",
    "        event_pre_df = pd.DataFrame(data={\n",
    "            'pre_event':1,\n",
    "            'time_period':[x.split('_')[-2] for x in event_pre_coef.index],\n",
    "            'treated_unit':[x.split('_')[-1] for x in event_pre_coef.index],\n",
    "            'coef_':event_pre_coef,\n",
    "            'se_':event_pre_se,\n",
    "            'tstat':event_pre_tstat,\n",
    "            'pvalue':event_pre_pvalues,\n",
    "            'FJointStat':FJointStat,\n",
    "            'FJointPValue':event_model.f_test(A).pvalue\n",
    "                              })\n",
    "        \n",
    "\n",
    "        ## Test whether all pre-trend estimates are different from zero\n",
    "        A = np.identity(len(event_model.params))\n",
    "        remove_row=[]\n",
    "        for i,name in zip(range(len(event_model.params)), event_model.params.index):\n",
    "            if 'pst' in name:\n",
    "                remove_row.append(i)\n",
    "            else:\n",
    "                pass\n",
    "        A = A[remove_row]\n",
    "        A = A[1:,:]     \n",
    "        \n",
    "        try:\n",
    "            FJointStat = event_model.f_test(A).statistic\n",
    "        except:\n",
    "            FJointStat = event_model.f_test(A).statistic.item()\n",
    "        try:\n",
    "            FJointPValue = event_model.f_test(A).pvalue\n",
    "        except:\n",
    "            FJointPValue = event_model.f_test(A).pvalue.item()        \n",
    "        event_pst_df = pd.DataFrame(data={\n",
    "            'pre_event':0,\n",
    "            'time_period':[ int(x.split('_')[-2]) for x in event_pst_coef.index],\n",
    "            'treated_unit':[ x.split('_')[-1] for x in event_pst_coef.index],\n",
    "            'coef_':event_pst_coef,\n",
    "            'se_':event_pst_se,\n",
    "            'tstat':event_pst_tstat,\n",
    "            'pvalue':event_pst_pvalues,\n",
    "            'FJointStat':FJointStat,\n",
    "            'FJointPValue':event_model.f_test(A).pvalue                   \n",
    "                              })\n",
    "\n",
    "        df_event_study_c = did.treated_counterfactual(did_model=event_model,\n",
    "                                        atet=event_pst_df.loc[event_pst_df.index.str.contains('pst')],\n",
    "                                        df = data,\n",
    "                                        data = event_X,\n",
    "                                        data_dict = data_dict)\n",
    "\n",
    "        \n",
    "        \n",
    "        return {'twfe':df_twfe, \n",
    "                'twfe_c': df_c,\n",
    "                'twfe_model':twfe_model,\n",
    "                'event_study':pd.concat([event_pre_df, event_pst_df]),\n",
    "                'event_study_c':df_event_study_c,\n",
    "               'event_study_model':event_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5938b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sdid:\n",
    "    \n",
    "    '''\n",
    "    **Final step is to estimate SDID given lambda and omega parameters.\n",
    "    '''\n",
    "    def twfe_sdid(data=None,\n",
    "            data_dict={'treatment':None,\n",
    "                      'date':None,\n",
    "                      'post':None,\n",
    "                      'unitid':None,\n",
    "                      'outcome':None}):\n",
    "        '''\n",
    "        Clean the dataset\n",
    "        '''\n",
    "        sc_dict = dgp.clean_and_input_data(dataset=data,\n",
    "                                           treatment=data_dict['treatment'],\n",
    "                                           unit_id = data_dict['unitid'],\n",
    "                                           date=data_dict['date'],\n",
    "                                           post=data_dict['post'],\n",
    "                                          outcome=data_dict['outcome'])        \n",
    "\n",
    "        \n",
    "        '''\n",
    "        Step 1 and 2 to estimate lambda and omega\n",
    "        '''\n",
    "        omega_weights = sdid.estimate_omega(sc_dict['C_pre'],\n",
    "                          sc_dict['C_pst'],\n",
    "                          sc_dict['T_pre'],\n",
    "                          sc_dict['T_pst'])\n",
    "        lambda_weights = sdid.estimate_lambda(sc_dict['C_pre'],\n",
    "                          sc_dict['C_pst'],\n",
    "                          sc_dict['T_pre'],\n",
    "                          sc_dict['T_pst'])        \n",
    "        ## Write the omega and lambda weights:\n",
    "        omega_df = pd.DataFrame()\n",
    "        lambda_df = pd.DataFrame()\n",
    "        for omega_i,omega_hat in zip(data[data_dict['unitid']].sort_values().unique(),omega_weights):\n",
    "            omega_df = pd.concat([omega_df,\\\n",
    "                            pd.DataFrame(index=[omega_i], data={'omega':omega_hat})])\n",
    "#             print('Omega for unit {0}: {1:5.3f}'.format(omega_i, omega_hat))\n",
    "        for lambda_t,lambda_hat in zip(data[data_dict['date']].sort_values().unique(),lambda_weights):\n",
    "            lambda_t = pd.to_datetime(lambda_t).strftime(\"%Y-%m-%d\")\n",
    "#             print('Lambda for time period {0}: {1:5.3f}'.format(lambda_t,\n",
    "#                                                                , lambda_hat))        \n",
    "            lambda_df = pd.concat([lambda_df,\\\n",
    "                            pd.DataFrame(index=[lambda_t], data={'lambda':lambda_hat})])\n",
    "\n",
    "        '''\n",
    "        Output the ATET from the final statsmodel, and estimate the counterfactual trend.\n",
    "        '''\n",
    "        ## Construct the TWFE regression by creating time indicators and unit indicators\n",
    "        ## This estimate treatment-unit specific treatment effects\n",
    "        treated_units = data.loc[data[data_dict['treatment']]==1][data_dict['unitid']].unique().tolist()\n",
    "        treated_na_replace = dict(zip(treated_units, [np.nan]*(len(treated_units))))\n",
    "        t_fe = pd.get_dummies(data[data_dict['date']]  ,drop_first=True)\n",
    "        x_fe = pd.get_dummies(data[data_dict['unitid']].replace(to_replace=treated_na_replace),\n",
    "                              drop_first=True, dummy_na=False)\n",
    "\n",
    "        ## Estimate a single ATET\n",
    "        ## so construct a single indicator \n",
    "        treated_units = data.loc[data[data_dict['treatment']]==1][data_dict['unitid']].unique().tolist()\n",
    "        post_treated = pd.DataFrame(\n",
    "            data={'post_SDiD': \n",
    "            (data[data_dict['post']]*(data[data_dict['unitid']].isin(treated_units))).astype(float)})\n",
    "\n",
    "\n",
    "        twfe_X = sm.add_constant( pd.concat([post_treated,  t_fe, x_fe ], axis=1)  )\n",
    "\n",
    "        ## Before we construct the inputs for the OLS model, we need to \n",
    "        ## weight all the observations by the omega and lambda weights.\n",
    "        ## To avoid errors from editting the dataframe, create a copy.\n",
    "        y_array = data[[data_dict['outcome']]].copy()\n",
    "        for omega_i,omega_hat in zip(data[ data_dict['unitid'] ].astype(int).sort_values().unique()\n",
    "                                     ,\n",
    "                                     omega_weights):\n",
    "            twfe_X.loc[data[ data_dict['unitid']]==omega_i ] *= omega_hat\n",
    "            y_array.loc[data[ data_dict['unitid']]==omega_i ] *= omega_hat       \n",
    "        for lambda_t,lambda_hat in zip(data[ data_dict['date'] ].sort_values().unique()\n",
    "                                       ,\n",
    "                                       lambda_weights):\n",
    "            twfe_X.loc[data[ data_dict['date']]==lambda_t ] *= lambda_hat\n",
    "            y_array.loc[data[ data_dict['date']]==lambda_t ] *= lambda_hat\n",
    "\n",
    "        twfe_model = sm.OLS(y_array,  twfe_X).fit()\n",
    "        twfe_coef = twfe_model.params.iloc[0:2]\n",
    "        twfe_se = twfe_model.bse.iloc[0:2]\n",
    "        twfe_tstat = twfe_model.tvalues.iloc[0:2]\n",
    "        twfe_pvalues = twfe_model.pvalues.iloc[0:2]\n",
    "\n",
    "        df_twfe = pd.DataFrame()\n",
    "        for r, coef_, se_, pv_ in zip(twfe_coef.index, twfe_coef, twfe_se, twfe_pvalues):\n",
    "            df_twfe = pd.concat([df_twfe,\n",
    "                                     pd.DataFrame(index=[r],\n",
    "                                                 data={\n",
    "                                                      'coef_':coef_,\n",
    "                                                      'se_':se_,\n",
    "                                                      'pvalue':pv_}  )])  \n",
    "        ## Predict counterfactual values\n",
    "        c_df = pd.DataFrame()\n",
    "        c_df['y'] = twfe_model.predict(twfe_X)\n",
    "        c_df[data_dict['date']] = data[data_dict['date']].values\n",
    "        c_df[data_dict['unitid']] = data[data_dict['unitid']].values    \n",
    "        c_df[data_dict['treatment']] = data[data_dict['treatment']].values    \n",
    "        c_df['post_SDiD'] = (data[data_dict['post']]*\\\n",
    "                             (data[data_dict['unitid']].isin(treated_units))).astype(float)\n",
    "        c_df['y_c'] = c_df['y'] - c_df['post_SDiD']*df_twfe['coef_']['post_SDiD']\n",
    "        c_df['y_obs'] = y_array.values\n",
    "        c_df['stder'] = df_twfe['se_']['post_SDiD']\n",
    "        return {'sdid':df_twfe, 'sdid_model':twfe_model,\n",
    "                    'omega_weights':omega_df,'lambda_weights':lambda_df,\n",
    "                'counterfactual':c_df}    \n",
    "    \n",
    "    '''\n",
    "    **Step 1** Estimate the regularization parameter    \n",
    "    '''\n",
    "    \n",
    "    def comp_reg_parameter(data_control_pre=None,\n",
    "                          data_control_pst=None,\n",
    "                          data_treat_pre=None,\n",
    "                          data_treat_pst=None):\n",
    "        T_post = data_control_pst.shape[0]\n",
    "        T_pre =  data_control_pre.shape[0]\n",
    "        N_tr = data_treat_pst.shape[1]\n",
    "        N_co = data_control_pst.shape[1]\n",
    "\n",
    "        ## Calculate the AR(1) period-to-period change per control units\n",
    "        delta_it = data_control_pre.shift(1).iloc[1:] - data_control_pre.iloc[1:]\n",
    "        delta_bar = delta_it.sum().sum()\n",
    "        delta_bar /= (N_co)*(T_pre-1)\n",
    "\n",
    "        ## Calculate the variance of AR(1) period-to-period change\n",
    "        sigma_hat = np.power( (delta_it - delta_bar), 2 ).sum().sum()\n",
    "        sigma_hat /= (N_co)*(T_pre-1)\n",
    "        zeta = np.power( N_tr * T_post , 0.25) * np.sqrt(sigma_hat)\n",
    "        return zeta    \n",
    "    \n",
    "    '''\n",
    "    **Step 2** Estimate the lambda parameter\n",
    "\n",
    "    **Step 3** Estimate the omega parameter. Keep in mind that the omega parameter is \n",
    "        estimated without the lambda parameter as an input. In other words, they are \n",
    "        estimated independently.    \n",
    "    '''\n",
    "    ## This function is used for optmization of lambda\n",
    "    def l_unit(omega_array = None,\n",
    "                zeta=0,\n",
    "               data_control_pre=None,\n",
    "                          data_control_pst=None,\n",
    "                          data_treat_pre=None,\n",
    "                          data_treat_pst=None):\n",
    "        omega_0 = omega_array[0]\n",
    "        omega_sdid = omega_array[1:]\n",
    "        N_tr = data_treat_pst.shape[1]\n",
    "\n",
    "        control_y = omega_0 + np.dot(data_control_pre, omega_sdid) \n",
    "        treat_y = data_treat_pre.sum(axis=1)\n",
    "        treat_y /= N_tr\n",
    "\n",
    "        regularization = zeta**2 * np.sum( omega_array**2 )\n",
    "        control_treat_y = np.sum( (omega_0 + control_y - treat_y)**2 )\n",
    "        return control_treat_y + regularization\n",
    "    def estimate_omega(data_control_pre=None,\n",
    "                          data_control_pst=None,\n",
    "                          data_treat_pre=None,\n",
    "                          data_treat_pst=None):\n",
    "\n",
    "        zeta_0 = sdid.comp_reg_parameter(data_control_pre=data_control_pre,\n",
    "                      data_control_pst=data_control_pst,\n",
    "                      data_treat_pre=data_treat_pre,\n",
    "                      data_treat_pst=data_treat_pst)\n",
    "        ## Initialize at a given point\n",
    "        initial_omega = np.ones(data_control_pre.shape[1]+1)/ data_control_pre.shape[1] \n",
    "\n",
    "        omega_weights = fmin_slsqp(partial(sdid.l_unit, \n",
    "                           zeta=zeta_0,\n",
    "                          data_control_pre=data_control_pre,\n",
    "                          data_control_pst=data_control_pst,\n",
    "                          data_treat_pre=data_treat_pre,\n",
    "                          data_treat_pst=data_treat_pst),\n",
    "                             initial_omega,\n",
    "                                 f_eqcons=lambda x: np.sum(x[1:]) - 1,\n",
    "                                   bounds=[(0.0, np.inf)]*len(initial_omega),\n",
    "                                 iter=50000, \n",
    "                                 disp=False)\n",
    "\n",
    "\n",
    "        return omega_weights    \n",
    "\n",
    "    def t_unit(lambda_array = None,\n",
    "                zeta=0,\n",
    "               data_control_pre=None,\n",
    "                          data_control_pst=None,\n",
    "                          data_treat_pre=None,\n",
    "                          data_treat_pst=None):\n",
    "        lambda_0 = lambda_array[0]\n",
    "        lambda_sdid = lambda_array[1:]\n",
    "\n",
    "        pre_y = lambda_0 + np.dot(lambda_sdid, data_control_pre)\n",
    "        pst_y = data_control_pst.sum(axis=0)\n",
    "        pst_y /= data_control_pst.shape[0]\n",
    "        ## We do regularization following footnote 3 from SDiD Paper\n",
    "        regularization = zeta**2*data_control_pre.shape[1]*np.sum( lambda_array**2 )\n",
    "\n",
    "        pre_pst_y = np.power(pre_y + pst_y, 2).sum() \n",
    "        return pre_pst_y\n",
    "\n",
    "\n",
    "    def estimate_lambda(data_control_pre=None,\n",
    "                          data_control_pst=None,\n",
    "                          data_treat_pre=None,\n",
    "                          data_treat_pst=None):\n",
    "\n",
    "        zeta_0 = sdid.comp_reg_parameter(data_control_pre=data_control_pre,\n",
    "                      data_control_pst=data_control_pst,\n",
    "                      data_treat_pre=data_treat_pre,\n",
    "                      data_treat_pst=data_treat_pst)\n",
    "        ## Initialize at a given point\n",
    "        initial_lambda = np.ones(data_control_pre.shape[0]+1)/ data_control_pre.shape[0] \n",
    "\n",
    "        lambda_weights = fmin_slsqp(partial(sdid.t_unit, \n",
    "                           zeta=zeta_0,\n",
    "                          data_control_pre=data_control_pre,\n",
    "                          data_control_pst=data_control_pst,\n",
    "                          data_treat_pre=data_treat_pre,\n",
    "                          data_treat_pst=data_treat_pst),\n",
    "                             initial_lambda,\n",
    "                                 f_eqcons=lambda x: np.sum(x[1:]) - 1,\n",
    "                                   bounds=[(0.0, np.inf)]*len(initial_lambda),\n",
    "                                 iter=50000, \n",
    "                                 disp=False)\n",
    "        return lambda_weights     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be348e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1fa4340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the univeral function that calls all the different SC models coded up below.\n",
    "## It will also call functions used for SC model validation\n",
    "\n",
    "class sc:\n",
    "    def sc_model(model_name='adh',\n",
    "                data=None,\n",
    "                data_dict= {'treatment':None, \n",
    "                            'date':None, \n",
    "                            'post':None, \n",
    "                            'unitid':None, \n",
    "                            'outcome':None},\n",
    "                pre_process_data=None,\n",
    "                pre_treatment_window=None,\n",
    "                aggregate_pst_periods=True,\n",
    "                inference={'alpha':0.05,\n",
    "                          'theta_grid':np.arange(-10,10,0.005)}):\n",
    "        ## First pre-process the data if possible.\n",
    "        if pre_process_data==None:\n",
    "            pre_process_data = dgp.clean_and_input_data(dataset=data,\n",
    "                                                        treatment=data_dict['treatment'],\n",
    "                                                        unit_id=data_dict['unitid'],\n",
    "                                                        date=data_dict['date'],\n",
    "                                                        post=data_dict['post'],\n",
    "                                                        outcome=data_dict['outcome'])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        ## Second calculate the pre-treatment-window that can be useful for placebo tests\n",
    "        pre_treatment_window = dgp.determine_pre_treatment_window(ci_data_output=pre_process_data,\n",
    "                                                                 pre_treatment_window=pre_treatment_window)\n",
    "        \n",
    "        \n",
    "        ## Finally, call an SC model.\n",
    "        if model_name=='adh':\n",
    "            print('Using ADH')\n",
    "            sc_est = adh.predict_omega(pre_process_data['T_pre'], \n",
    "                                    pre_process_data['C_pre'], \n",
    "                                    pre_treatment_window)\n",
    "            sc_output = di.sc_style_results(pre_process_data['T_pre'], \n",
    "                                             pre_process_data['T_pst'],\n",
    "                                             pre_process_data['C_pre'], \n",
    "                                             pre_process_data['C_pst'],\n",
    "                                np.zeros(pre_process_data['T_pst'].shape[1]), np.array(sc_est['omega']))\n",
    "            sc_est['mu'] = np.zeros( pre_process_data['T_pre'].shape[1] )\n",
    "            \n",
    "        elif model_name=='di':\n",
    "            print('Using DI')\n",
    "            w=alpha_lambda.get_alpha_lambda(sc_dict['C_pre'])\n",
    "            alpha_lambda_to_use = alpha_lambda.alpha_lambda_transform(w.x)\n",
    "            ## Take the alpha and lambda values, and estimate mu and omega\n",
    "            sc_est = di.predict_mu_omega(pre_process_data['T_pre'], \n",
    "                                         pre_process_data['C_pre'], alpha_lambda_to_use, \n",
    "                                         pre_treatment_window)\n",
    "            sc_output = di.sc_style_results(pre_process_data['T_pre'], \n",
    "                                            pre_process_data['T_pst'],\n",
    "                                            pre_process_data['C_pre'], \n",
    "                                            pre_process_data['C_pst'],\n",
    "                                            sc_est['mu'],sc_est['omega'])\n",
    "            \n",
    "        elif model_name=='cl':\n",
    "            print('Using CL')\n",
    "            sc_est = cl.predict_mu_omega(pre_process_data['T_pre'],\n",
    "                                      pre_process_data['C_pre'], \n",
    "                                      pre_treatment_window)\n",
    "            sc_output = di.sc_style_results(pre_process_data['T_pre'],\n",
    "                                            pre_process_data['T_pst'],\n",
    "                                            pre_process_data['C_pre'], \n",
    "                                            pre_process_data['C_pst'],\n",
    "                                            sc_est['mu'], sc_est['omega']) \n",
    "        else:\n",
    "            print('SC Model name not supported. \\n [adh,di,cl] are supported models.')\n",
    "        \n",
    "#         ## Output measures of fit pre-treatment (training), pre_treatment (test), and post-treatment\n",
    "#         sc_validation = sc.sc_validation(treatment_pre=pre_process_data['T_pre'], \n",
    "#                      treatment_pst=pre_process_data['T_pst'],\n",
    "#                      control_pre=  pre_process_data['C_pre'],\n",
    "#                      control_pst=  pre_process_data['C_pst'], \n",
    "#                      mu=  sc_est['mu'],\n",
    "#                      omega=sc_est['omega'],\n",
    "#                      pre_treatment_window=pre_treatment_window)\n",
    "        \n",
    "        ## Improve on this by calling for inference results\n",
    "        sc_df_results = sc.collect_sc_outputs(sc_output = sc_output,\n",
    "                           pre_process_data=pre_process_data,\n",
    "                       theta_grid = inference['theta_grid'],\n",
    "                           pre_treatment_window=pre_treatment_window, \n",
    "                                            aggregate_pst_periods=aggregate_pst_periods,\n",
    "                                              alpha = inference['alpha'])\n",
    "        \n",
    "        return {**sc_output, 'sc_est':sc_est, 'results_df':sc_df_results}\n",
    "\n",
    "\n",
    "    '''\n",
    "    Create a function that evaluates how well the SC model does at matching the:\n",
    "    1. pre-treatment data used for training;\n",
    "    2. pre-treatment data used for testing; and\n",
    "    3. the post-treatment data.\n",
    "    \n",
    "    Calculate metrics of fit and compare pre-treatment and post-treatment metrics.\n",
    "    '''\n",
    "    def sc_validation(treatment_pre, treatment_pst, control_pre, control_pst, \n",
    "                                      mu,omega,\n",
    "                                     pre_treatment_window):\n",
    "        ## Re-combine the observed treatment and control outcomes\n",
    "        y_treat_obs = pd.concat([treatment_pre, treatment_pst], axis=0)\n",
    "        y_control_obs = pd.concat([control_pre, control_pst], axis=0)\n",
    "\n",
    "        ## Estimate the counterfactual outcome of the treatment group\n",
    "        y_treat_hat = mu + np.dot(y_control_obs, omega.T)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_percentage_error\n",
    "        def comparison_over_windows(x,y,\n",
    "                                    pre_treatment_window,\n",
    "                                   metric_func, index_name):\n",
    "            x_pre_train = x[0:pre_treatment_window[0]]\n",
    "            y_pre_train = y[0:pre_treatment_window[0]]\n",
    "            x_pre_test = x[pre_treatment_window[0]:pre_treatment_window[0]+pre_treatment_window[1]]\n",
    "            y_pre_test = y[pre_treatment_window[0]:pre_treatment_window[0]+pre_treatment_window[1]]\n",
    "            x_pst_test = x[-1*(pre_treatment_window[0]+pre_treatment_window[1]):].copy()\n",
    "            y_pst_test = y[-1*(pre_treatment_window[0]+pre_treatment_window[1]):].copy()\n",
    "            test_pre_train = metric_func(x_pre_train, y_pre_train)\n",
    "            test_pre_test  = metric_func(x_pre_test,  y_pre_test)\n",
    "            test_pst_test  = metric_func(x_pst_test,  y_pst_test)\n",
    "            \n",
    "            return pd.DataFrame(index=[index_name], data={'test_pre_train':test_pre_train,\n",
    "                                                          'test_pre_train_N':pre_treatment_window[0],\n",
    "                   'test_pre_test':test_pre_test,\n",
    "                   'test_pre_test_N':pre_treatment_window[1],\n",
    "                   'test_pst_test':test_pst_test,\n",
    "                    'test_pst_test_N':len(y_pst_test)})\n",
    "\n",
    "        ## Compare the predicted treatment with the observed treatment\n",
    "        treat_hat_treat_obs = comparison_over_windows(y_treat_hat, y_treat_obs,\n",
    "                                                   pre_treatment_window,\n",
    "                                                   mean_absolute_percentage_error,'mape_vs_treat_obs')\n",
    "        return treat_hat_treat_obs\n",
    "\n",
    "    def sc_validation_gather(counterfactual=None,\n",
    "                              actual=None,\n",
    "                             pre_treatment_window=None):\n",
    "        from sklearn.metrics import mean_absolute_percentage_error        \n",
    "\n",
    "        x_pre_train = counterfactual[0:pre_treatment_window[0]]\n",
    "        y_pre_train = actual[0:pre_treatment_window[0]]\n",
    "        x_pre_test = counterfactual[pre_treatment_window[0]:pre_treatment_window[0]+pre_treatment_window[1]]\n",
    "        y_pre_test = actual[pre_treatment_window[0]:pre_treatment_window[0]+pre_treatment_window[1]]\n",
    "        x_pst_test = counterfactual[-1*(pre_treatment_window[0]+pre_treatment_window[1]):].copy()\n",
    "        y_pst_test = actual[-1*(pre_treatment_window[0]+pre_treatment_window[1]):].copy()\n",
    "        test_pre_train = mean_absolute_percentage_error(x_pre_train, y_pre_train)\n",
    "        test_pre_test  = mean_absolute_percentage_error(x_pre_test,  y_pre_test)\n",
    "        test_pst_test  = mean_absolute_percentage_error(x_pst_test,  y_pst_test)\n",
    "            \n",
    "        return pd.DataFrame(index=['mape_test'], data={'test_pre_train':test_pre_train,\n",
    "               'test_pre_test':test_pre_test,\n",
    "               'test_pst_test':test_pst_test})\n",
    "\n",
    "    '''\n",
    "    Output a dataframe collecting different metrics for each treated unit,\n",
    "    such as the ATET, p-value, confidence interval, and placebo tests.\n",
    "    \n",
    "    All confidence intervals and p-values to be calculated for individual post-treatment periods\n",
    "    '''\n",
    "    def collect_sc_outputs(sc_output = None,\n",
    "                           pre_process_data=None,\n",
    "                           theta_grid = None,  # np.arange(-10,10,0.5),\n",
    "                           pre_treatment_window=None,\n",
    "                           aggregate_pst_periods=True,\n",
    "                      alpha = 0.05):\n",
    "        \n",
    "        if aggregate_pst_periods==True:\n",
    "            a = sc.collect_sc_outputs_aggregate(sc_output = sc_output,\n",
    "                           pre_process_data=pre_process_data,\n",
    "                           theta_grid = theta_grid,  # np.arange(-10,10,0.5),\n",
    "                           pre_treatment_window=pre_treatment_window,\n",
    "                      alpha = 0.05)\n",
    "        else:\n",
    "            a = sc.collect_sc_outputs_individual(sc_output = sc_output,\n",
    "               pre_process_data=pre_process_data,\n",
    "               theta_grid = theta_grid,  # np.arange(-10,10,0.5),\n",
    "               pre_treatment_window=pre_treatment_window,\n",
    "              alpha = 0.05)\n",
    "        return a\n",
    "    def collect_sc_outputs_individual(sc_output = None,\n",
    "                           pre_process_data=None,\n",
    "                           theta_grid = None,  # np.arange(-10,10,0.5),\n",
    "                           pre_treatment_window=None,\n",
    "                           aggregate_pst_periods=True,\n",
    "                      alpha = 0.05):\n",
    "        ## To do the individual results, just call the function that does that aggregated\n",
    "        ## results multiple times, assuming that there is only one post-treatment period.\n",
    "        \n",
    "        ## Calculate some primatives:\n",
    "        pre_T = pre_process_data['T_pre'].shape[0]\n",
    "        pst_T = pre_process_data['T_pst'].shape[0]\n",
    "        \n",
    "        \n",
    "        ## Create all permutations over all per-treatment periods and just one post-treatment period\n",
    "        permutations_subset_block_individual = []\n",
    "        individual_time_list = np.arange( pre_treatment_window[0]+1 )        \n",
    "        for i in range(pre_T+1):\n",
    "            half_A = individual_time_list[-1*(pre_T-i):].copy()\n",
    "            half_B = individual_time_list[0:i]\n",
    "            scrambled_list = np.concatenate([half_A, half_B]) \n",
    "            permutations_subset_block_individual.append( list(scrambled_list)  )\n",
    "        \n",
    "        ## Go through different permutations of pre- and post-periods to do inference\n",
    "        collect_individual_df = pd.DataFrame()\n",
    "        for t_pst in range(0, pst_T):\n",
    "#             print(pre_process_data['treatment_window'])\n",
    "            pst_index = np.append(np.arange(pre_T) ,[pre_T+t_pst]) \n",
    "\n",
    "            sc_output_subset = {'atet':sc_output['atet'].iloc[t_pst],\n",
    "                               'predict_est':sc_output['predict_est'].iloc[pst_index]}\n",
    "            pre_process_data_subset = {'time_scramble': permutations_subset_block_individual,\n",
    "                                      'treatment_window':(pre_process_data['treatment_window'][0]\n",
    "                                                         ,\n",
    "                                                         1)}\n",
    "            a = sc.collect_sc_outputs_aggregate(sc_output = sc_output_subset,\n",
    "                                       pre_process_data=pre_process_data_subset,\n",
    "                                       theta_grid = theta_grid,\n",
    "                                       pre_treatment_window=(pre_process_data['treatment_window'][0]\n",
    "                                                         ,\n",
    "                                                         1),\n",
    "                                  alpha = 0.05,\n",
    "                                               time_period_forindividual=t_pst)\n",
    "            a['individual_post']     = t_pst\n",
    "            collect_individual_df = pd.concat([collect_individual_df, a])\n",
    "        return collect_individual_df\n",
    "    \n",
    "    def collect_sc_outputs_aggregate(sc_output = None,\n",
    "                           pre_process_data=None,\n",
    "                           theta_grid = None,  # np.arange(-10,10,0.5),\n",
    "                           pre_treatment_window=None,\n",
    "                          alpha = 0.05,\n",
    "                                    time_period_forindividual=None): \n",
    "        ## Alter this function to collect information that is aggregated across all time periods,\n",
    "        ## and for information that is for a specific time period.\n",
    "        if len(sc_output['atet'].shape) > 1 :\n",
    "            sc_results = sc_output['atet'].mean(axis=0).copy().to_frame().rename(columns={0:'atet'})\n",
    "        else:\n",
    "            number_of_treated_units = len([x for x in sc_output['predict_est'].columns if '_est' not in x])  \n",
    "            sc_results = pd.DataFrame(index=[np.arange(number_of_treated_units)], \n",
    "                                      data={'time_period':time_period_forindividual,\n",
    "                                            'atet': sc_output['atet'].values })\n",
    "            \n",
    "        sc_pv = []\n",
    "        sc_ci_05 = []\n",
    "        sc_ci_95 = []\n",
    "        sc_se = []\n",
    "        sc_placebo_pre_train = []\n",
    "        sc_placebo_pre_test = []\n",
    "        sc_placebo_pst_test = []\n",
    "            \n",
    "        o = 0\n",
    "        for p in [x for x in sc_output['predict_est'].columns if '_est' not in x]:\n",
    "            ## Calculate the p-value\n",
    "            pv_output = conformal_inf.pvalue_calc(counterfactual=np.array( sc_output['predict_est']['{0}_est'.format(p)].tolist() ),\n",
    "                                      actual=np.array( sc_output['predict_est']['{0}'.format(p)].tolist() ),\n",
    "                                      permutation_list =pre_process_data['time_scramble'],\n",
    "                                      treatment_window = pre_process_data['treatment_window'],\n",
    "                                      h0=0)\n",
    "            sc_pv.append(pv_output)\n",
    "\n",
    "            ## Calculate the confidence interval\n",
    "            ## If no pre-defined theta grid is defined, then just look 100 values in either direction\n",
    "            ## of the ATET estimate\n",
    "            if theta_grid is None:\n",
    "                i = sc_results['atet'].iloc[o]/100\n",
    "                theta_grid_use = np.arange(-500*i+sc_results['atet'].iloc[o],\n",
    "                                      -500*i+sc_results['atet'].iloc[o], i*5 )            \n",
    "            else:\n",
    "                theta_grid_use = theta_grid.copy()\n",
    "            \n",
    "            ci_output = conformal_inf.ci_calc(y_hat=sc_output['predict_est']['{0}_est'.format(p)].values,\n",
    "                           y_act=sc_output['predict_est']['{0}'.format(p)].values,\n",
    "                           theta_grid=theta_grid_use,\n",
    "                           permutation_list_ci =pre_process_data['time_scramble'],\n",
    "                           treatment_window_ci = pre_process_data['treatment_window'],\n",
    "                           alpha=alpha)\n",
    "#             print( ci_output['theta_list'] )\n",
    "#             print( ci_output['pvalue_list'] )\n",
    "            sc_ci_05.append(ci_output['ci_interval'][0])\n",
    "            sc_ci_95.append(ci_output['ci_interval'][1])\n",
    "\n",
    "            ## Calculate the placebo tests for that treated unit too\n",
    "            placebo_df = sc.sc_validation_gather(counterfactual=np.array( sc_output['predict_est']['{0}_est'.format(p)].tolist() ),\n",
    "                                      actual=np.array( sc_output['predict_est']['{0}'.format(p)].tolist() ),\n",
    "                                     pre_treatment_window=pre_process_data['treatment_window'])\n",
    "            sc_placebo_pre_train.append(placebo_df['test_pre_train'].values[0])\n",
    "            sc_placebo_pre_test.append( placebo_df['test_pre_test'].values[0])\n",
    "            sc_placebo_pst_test.append( placebo_df['test_pst_test'].values[0]) \n",
    "            o+=1\n",
    "\n",
    "        sc_results['pvalues'] = sc_pv\n",
    "        sc_results['ci_lower'] = sc_ci_05\n",
    "        sc_results['ci_upper'] = sc_ci_95\n",
    "        sc_results['alpha'] = alpha\n",
    "        sc_results['test_pre_train_MAPE'] = sc_placebo_pre_train\n",
    "        sc_results['test_pre_test_MAPE'] = sc_placebo_pre_test\n",
    "        sc_results['test_pst_test_MAPE'] = sc_placebo_pst_test\n",
    "        \n",
    "        return sc_results\n",
    "\n",
    "\n",
    "    def sc_generate_figures(final_sc_output=None,\n",
    "                           output_figure_name=None):\n",
    "        def check_write_file(suffix=None):\n",
    "            if output_figure_name==None:\n",
    "                pass\n",
    "            else:\n",
    "                plt.savefig(output_figure_name+'_{0}.png'.format(suffix))\n",
    "\n",
    "        ## Single Picture\n",
    "        fig,ax = plt.subplots(ncols=1,nrows=1, figsize=(12,6))\n",
    "        for r in [o for o in final_sc_output['predict_est'].columns if '_est' not in o]:\n",
    "            c = ax.plot(final_sc_output['predict_est'].index,final_sc_output['predict_est'][r+'_est'],\n",
    "                        color=None,\n",
    "                        linestyle='--',\n",
    "                        label='Estimated Control for {0}'.format(r),\n",
    "                   marker='o', markerfacecolor='white',markeredgecolor=None)\n",
    "            t = ax.plot(final_sc_output['predict_est'].index,final_sc_output['predict_est'][r],\n",
    "                        color=c[0].get_color(),\n",
    "                        label='Observed for {0}'.format(r),\n",
    "                   marker='o', markerfacecolor=c[0].get_color(), markeredgecolor=c[0].get_color())\n",
    "            \n",
    "        xmin_, xmax_, ymin_, ymax_ = plt.axis()\n",
    "        ax.set_xlabel('Time Periods')\n",
    "        ax.set_ylabel('Outcome')\n",
    "        ax.vlines(x=final_sc_output['atet'].index.min(), \n",
    "                  ymin=ymin_, ymax=ymax_, \n",
    "                  color='black',\n",
    "                  linewidth=2,\n",
    "                 label='Treatment Time')\n",
    "        ax.grid()\n",
    "        plt.legend()\n",
    "        check_write_file(suffix='overall')\n",
    "        plt.show()\n",
    "\n",
    "        ## Picture for each Treated Unit\n",
    "        number_of_treated_units = len([o for o in final_sc_output['predict_est'].columns if '_est' not in o])\n",
    "        fig,ax = plt.subplots(ncols=1,nrows=number_of_treated_units, figsize=(12,6*number_of_treated_units))\n",
    "        for i,r in zip(range(number_of_treated_units),\n",
    "                       [o for o in final_sc_output['predict_est'].columns if '_est' not in o]):\n",
    "            c = ax[i].plot(final_sc_output['predict_est'].index,final_sc_output['predict_est'][r+'_est'],\n",
    "                        color=None,\n",
    "                        linestyle='--',\n",
    "                        label='Estimated Control for {0}'.format(r),\n",
    "                   marker='o', markerfacecolor='white',markeredgecolor=None)\n",
    "            t = ax[i].plot(final_sc_output['predict_est'].index,final_sc_output['predict_est'][r],\n",
    "                        color=c[0].get_color(),\n",
    "                        label='Observed for {0}'.format(r),\n",
    "                   marker='o', markerfacecolor=c[0].get_color(), markeredgecolor=c[0].get_color())\n",
    "            xmin_, xmax_, ymin_, ymax_ = ax[i].axis()\n",
    "            ax[i].set_xlabel('Time Periods')\n",
    "            ax[i].set_ylabel('Outcome')\n",
    "            ax[i].vlines(x=final_sc_output['atet'].index.min(), \n",
    "                      ymin=ymin_, ymax=ymax_, \n",
    "                      color='black',\n",
    "                      linewidth=2,\n",
    "                     label='Treatment Time')    \n",
    "            ax[i].grid()\n",
    "            ax[i].legend()\n",
    "            ax[i].set_title('Estimated and Observed Trends for {0}'.format(r))\n",
    "        check_write_file(suffix='deep_dive')        \n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "## DGP and functions to determine what types of SC models to do, and calculate primatives for SC models\n",
    "class dgp:    \n",
    "    def determine_pre_treatment_window(ci_data_output=None,\n",
    "                                      pre_treatment_window=None):\n",
    "        if pre_treatment_window ==None:\n",
    "            pre_treatment_len = ci_data_output['C_pre'].shape[0]\n",
    "            pre_t0 = int( 0.75*pre_treatment_len )\n",
    "            if pre_t0 < 1:\n",
    "                pre_t0=1\n",
    "            else:\n",
    "                pass            \n",
    "            pre_t1 = pre_treatment_len - pre_t0\n",
    "            pre_treatment_window = [pre_t0, pre_t1]\n",
    "        else:\n",
    "            pass\n",
    "        return pre_treatment_window \n",
    "    \n",
    "    def clean_and_input_data(dataset=None, \n",
    "                             treatment='treated_unit', \n",
    "                             unit_id = 'unitid',\n",
    "                             date='T',\n",
    "                             post='post', outcome='Y'):\n",
    "        \n",
    "        C_pre = dataset.loc[(dataset[treatment]==0) & (dataset[post]==0)].pivot_table(columns=unit_id,\n",
    "                                                index=date,\n",
    "                                                values=outcome)\n",
    "        C_pst = dataset.loc[(dataset[treatment]==0) & (dataset[post]==1)].pivot_table(columns=unit_id,\n",
    "                                                index=date,\n",
    "                                                values=outcome)\n",
    "        T_pre = dataset.loc[(dataset[treatment]==1) & (dataset[post]==0)].pivot_table(columns=unit_id,\n",
    "                                                index=date,\n",
    "                                                values=outcome)\n",
    "        T_pst = dataset.loc[(dataset[treatment]==1) & (dataset[post]==1)].pivot_table(columns=unit_id,\n",
    "                                                index=date,\n",
    "                                                values=outcome)\n",
    "        \n",
    "        permutations_subset_block = conformal_inf.time_block_permutation(data=dataset, \n",
    "                                                                         time_unit=date,\n",
    "                                                                         post=post)\n",
    "                \n",
    "        return {'C_pre':C_pre, 'C_pst':C_pst, 'T_pre':T_pre, 'T_pst':T_pst, \n",
    "                'time_scramble':permutations_subset_block[0],\n",
    "               'treatment_window':permutations_subset_block[1]}\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6a44a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doudchenko Imbens, (2016) Model\n",
    "class di:\n",
    "    def estimate_mu_omega(treatment_pre, control_pre, alpha_lambda_0):\n",
    "        alpha_0, lambda_0 = alpha_lambda_0[0], alpha_lambda_0[1]\n",
    "        elnet = ElasticNet(random_state=2736, alpha=alpha_0, l1_ratio=lambda_0)\n",
    "        elnet.fit(control_pre, treatment_pre )\n",
    "        ## Output interpretable weights\n",
    "        try:\n",
    "            df_weights= pd.DataFrame(data=zip(treatment_pre.columns,\n",
    "                                              elnet.coef_.T\n",
    "                                             ))\n",
    "        except:\n",
    "            df_weights = pd.DataFrame(index=np.arange(len(elnet.coef_)), \n",
    "                         data=elnet.coef_.T)        \n",
    "        return {'mu': elnet.intercept_, 'omega': elnet.coef_, 'weights':df_weights, 'full':elnet}\n",
    "\n",
    "    def predict_mu_omega(treatment_pre, control_pre, alpha_lambda_0, holdout_windows):\n",
    "        ## Don't use all the control data\n",
    "        ## Make sure that the holdout windows add up to the total number of pre-treatment units\n",
    "        if (holdout_windows[0]+holdout_windows[1] != len(control_pre)):\n",
    "            print('the arg holdout_windows does not add up to the number of time units!')\n",
    "            print('holdout_windows = {0}'.format(holdout_windows))\n",
    "            print('total number of time periods = {0}'.format(len(control_pre)))\n",
    "        else:\n",
    "            pass    \n",
    "        ## Define the holdout samples\n",
    "        control_holdout = control_pre[0:holdout_windows[0]].copy()\n",
    "        treatment_holdout = treatment_pre[0:holdout_windows[0]] .copy()   \n",
    "        \n",
    "        control_nonholdout = control_pre[holdout_windows[0]:].copy()\n",
    "        treatment_nonholdout = treatment_pre[holdout_windows[0]:].copy()\n",
    "        \n",
    "        ## Estimate the DI model\n",
    "        holdout_dict = di.estimate_mu_omega(treatment_holdout, control_holdout, alpha_lambda_0)\n",
    "        if treatment_pre.shape[1]==1:\n",
    "            holdout_dict['omega'] = np.array([holdout_dict['omega']])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        ## Estimate measure of fit for the hold out and non-holdout sample\n",
    "        diff_holdout = treatment_holdout       -\\\n",
    "            np.dot(control_holdout, holdout_dict['omega'].T)+holdout_dict['mu']        \n",
    "        diff_nonholdout = treatment_nonholdout -\\\n",
    "            np.dot(control_nonholdout, holdout_dict['omega'].T)+holdout_dict['mu']\n",
    "\n",
    "        diff_nonholdout_mse = (diff_nonholdout**2).mean()\n",
    "        diff_holdout_mse = (diff_holdout**2).mean()\n",
    "        return {'mu':     holdout_dict['mu'],\n",
    "               'omega':   holdout_dict['omega'],\n",
    "               'weights': holdout_dict['weights'],\n",
    "               'full':    holdout_dict['full'],\n",
    "               'mse_holdout': diff_holdout_mse,\n",
    "               'mse_nonholdout':diff_nonholdout_mse}\n",
    "    \n",
    "    def sc_style_results(treatment_pre, treatment_pst, control_pre, control_pst, mu,omega):\n",
    "        ## Do some normalization of the omega input\n",
    "        if len(omega.shape) > 2:\n",
    "            omega = omega.reshape(omega.shape[-2],omega.shape[-1])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        final_X = pd.concat([treatment_pre, treatment_pst], axis=0)\n",
    "        control_X = pd.concat([control_pre, control_pst], axis=0)\n",
    "\n",
    "        control_df = mu + pd.DataFrame(data=np.dot(control_X, omega.T), columns=[ l+'_est' for l in final_X.columns ])\n",
    "        control_df.index = final_X.index\n",
    "        \n",
    "        output_df = control_df.join(final_X)\n",
    "        \n",
    "        treatment_periods = -1*len(treatment_pst)\n",
    "        atet_df = pd.DataFrame()\n",
    "        for c in [l for l in output_df.columns if '_est' not in l]:\n",
    "            diff = output_df[c][treatment_periods:].copy() - output_df[c+'_est'][treatment_periods:].copy()\n",
    "            atet_df[c] = diff\n",
    "        \n",
    "        return {'atet': atet_df , 'predict_est':output_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5ca4ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here code up the functions that we will try to minimize over\n",
    "class alpha_lambda:\n",
    "    def diff(y_t,y_c, mu_x, omega_x):\n",
    "        return y_t - mu_x - np.dot(y_c,omega_x)\n",
    "\n",
    "    def alpha_lambda_transform(alpha_lambda_raw_):\n",
    "        ## Alpha is strictly greater than zero\n",
    "        ## lambda exists between 0 and 1\n",
    "        return np.exp(alpha_lambda_raw_[0]/1000), np.exp(alpha_lambda_raw_[1])/(1+np.exp(alpha_lambda_raw_[1])), \n",
    "    def alpha_lambda_diff(alpha_lambda_raw_, control_pre):\n",
    "        ## Transform the inputted alpha,lambda values \n",
    "        alpha_lambda_t = alpha_lambda.alpha_lambda_transform(alpha_lambda_raw_)\n",
    "        difference_array = []\n",
    "        ## Pick one control unit as the pretend treatment unit\n",
    "        for u in control_pre.columns:\n",
    "            control_pre_placebo_treat = control_pre[u]\n",
    "            control_pre_placebo_cntrl = control_pre[ [l for l in control_pre.columns if l !=u] ]\n",
    "\n",
    "            ## Estimate mu and lambda with that control unit\n",
    "            control_pre_placebo_w = di.estimate_mu_omega(control_pre_placebo_treat,\n",
    "                             control_pre_placebo_cntrl,\n",
    "                             alpha_lambda_t)\n",
    "            ## Estimate the difference\n",
    "            d = alpha_lambda.diff(control_pre_placebo_treat, \n",
    "                 control_pre_placebo_cntrl, \n",
    "                 control_pre_placebo_w['mu'],\n",
    "                 control_pre_placebo_w['omega'])\n",
    "            difference_array.append(d)\n",
    "        ## Estimate the difference across all the control units\n",
    "        d_mean = np.mean(difference_array)\n",
    "        return d_mean\n",
    "    def get_alpha_lambda(control_pre_input):\n",
    "        ## Initialize at a given point\n",
    "        weights = minimize(partial(alpha_lambda.alpha_lambda_diff, control_pre=control_pre_input),\n",
    "                             np.array([10.15,0.5]),\n",
    "                           method='BFGS',\n",
    "                          options={'maxiter':5000, 'gtol': 1e-07, 'disp':False})\n",
    "        return weights\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "488a1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constrained Lasso Model\n",
    "from scipy.optimize import fmin_slsqp\n",
    "\n",
    "class cl:\n",
    "    def cl_obj(params, y,x) -> float:\n",
    "        return np.mean(  (y - params[0] - np.dot(x,params[1:]))**2)\n",
    "    \n",
    "    def predict_mu_omega(treatment_pre, control_pre, holdout_windows):\n",
    "        ## Don't use all the control data\n",
    "        ## Make sure that the holdout windows add up to the total number of pre-treatment units\n",
    "        if (holdout_windows[0]+holdout_windows[1] != len(control_pre)):\n",
    "            print('the arg holdout_windows does not add up to the number of time units!')\n",
    "            print('holdout_windows = {0}'.format(holdout_windows))\n",
    "            print('total number of time periods = {0}'.format(len(control_pre)))\n",
    "        else:\n",
    "            pass    \n",
    "        ## Define the holdout samples\n",
    "        control_holdout = control_pre[0:holdout_windows[0]].copy()\n",
    "        treatment_holdout = treatment_pre[0:holdout_windows[0]].copy()    \n",
    "        \n",
    "        control_nonholdout = control_pre[holdout_windows[0]:].copy()\n",
    "        treatment_nonholdout = treatment_pre[holdout_windows[0]:].copy()\n",
    "        \n",
    "        ## Estimate the CL model\n",
    "        ## Let's loop over different treatment units.\n",
    "        holdout_dict = {}\n",
    "        holdout_dict['mu'] = []\n",
    "        holdout_dict['omega'] = []\n",
    "        holdout_dict['weights'] = []\n",
    "        diff_holdout_mse = []\n",
    "        diff_nonholdout_mse = []\n",
    "        if treatment_pre.shape[1] > 1:\n",
    "            for t in treatment_pre.columns:\n",
    "                t_dict = cl.get_mu_omega(treatment_holdout[t], control_holdout)\n",
    "                ## Estimate measure of fit for the hold out and non-holdout sample\n",
    "                diff_h = treatment_holdout[t]       - np.dot(control_holdout, t_dict['omega'].T)+t_dict['mu']\n",
    "                diff_h_mse = (diff_h**2).mean()\n",
    "                diff_nh = treatment_nonholdout[t] - np.dot(control_nonholdout, t_dict['omega'].T)+t_dict['mu']\n",
    "                diff_nh_mse = (diff_nh**2).mean()\n",
    "        \n",
    "                holdout_dict['mu'].append(t_dict['mu'])\n",
    "                holdout_dict['omega'].append(t_dict['omega'])\n",
    "                holdout_dict['weights'].append(t_dict['weights'])\n",
    "                diff_holdout_mse.append(diff_h_mse)\n",
    "                diff_nonholdout_mse.append(diff_nh_mse)\n",
    "        else:\n",
    "            \n",
    "            t_dict = cl.get_mu_omega(treatment_holdout.values.flatten(), control_holdout)\n",
    "            ## Estimate measure of fit for the hold out and non-holdout sample\n",
    "            t_dict['omega'] = np.array([t_dict['omega']])\n",
    "            diff_h= treatment_holdout       - np.dot(control_holdout, t_dict['omega'].T)+t_dict['mu']\n",
    "            diff_h_mse = (diff_h**2).mean()\n",
    "            diff_nh = treatment_nonholdout - np.dot(control_nonholdout, t_dict['omega'].T)+t_dict['mu']\n",
    "            diff_nh_mse = (diff_nh**2).mean()\n",
    "\n",
    "            holdout_dict['mu'].append(t_dict['mu'])\n",
    "            holdout_dict['omega'].append(t_dict['omega'])\n",
    "            holdout_dict['weights'].append(t_dict['weights'])\n",
    "            diff_holdout_mse.append(diff_h_mse)\n",
    "            diff_nonholdout_mse.append(diff_nh_mse)            \n",
    "                    \n",
    "        holdout_dict['omega'] = np.array(holdout_dict['omega'])\n",
    "        if len(holdout_dict['omega'].shape) > 2:\n",
    "            holdout_dict['omega'] = holdout_dict['omega'].reshape(holdout_dict['omega'].shape[-2],holdout_dict['omega'].shape[-1])\n",
    "        else:\n",
    "            pass        \n",
    "        holdout_dict['mse_holdout'] =np.mean(diff_holdout_mse)\n",
    "        holdout_dict['mse_nonholdout'] =np.mean(diff_holdout_mse)        \n",
    "\n",
    "        return holdout_dict\n",
    "\n",
    "\n",
    "    def get_mu_omega(treatment_pre_input, control_pre_input):\n",
    "        n = control_pre_input.shape[1]\n",
    "        initialx = np.ones(n+1)/1\n",
    "        ## Initialize at a given point\n",
    "        weights = fmin_slsqp(partial(cl.cl_obj, y=treatment_pre_input,\n",
    "                                  x=control_pre_input),\n",
    "                             initialx,\n",
    "                             f_ieqcons=lambda x: 1-np.sum(np.abs(x[1:])),\n",
    "                         iter=50000, \n",
    "                         disp=False)\n",
    "        mu, omega = weights[0], weights[1:]\n",
    "        return {'mu':mu, 'omega':omega, 'weights':weights}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2d4eb46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Abadie, Diamond, Hainmueller (2010) model\n",
    "## Also code up the ADH weights\n",
    "## Abadie/Diamond/Hainmueller    \n",
    "from typing import List\n",
    "from operator import add\n",
    "from toolz import reduce, partial\n",
    "from scipy.optimize import fmin_slsqp\n",
    "\n",
    "class adh:\n",
    "    ## Define loss function\n",
    "    def loss_w(W, X, y) -> float:\n",
    "        return np.sqrt(np.mean((y - X.dot(W))**2))\n",
    "\n",
    "    def get_w(X, y):\n",
    "        ## Initialize at sample average with some noise\n",
    "        w_start = [1/X.shape[1]]*X.shape[1]\n",
    "    #     w_start = np.ones(X.shape[1])\n",
    "\n",
    "        weights = fmin_slsqp(partial(adh.loss_w, X=X, y=y),\n",
    "                             np.array(w_start),\n",
    "                             f_eqcons=lambda x: np.sum(x) - 1,\n",
    "                             iter=50000, \n",
    "                             bounds=[(0.0, 1.0)]*len(w_start),\n",
    "                             disp=False)\n",
    "        return weights   \n",
    "    \n",
    "    def predict_omega(treatment_pre, control_pre, holdout_windows):\n",
    "        ## Don't use all the control data\n",
    "        ## Make sure that the holdout windows add up to the total number of pre-treatment units\n",
    "        if (holdout_windows[0]+holdout_windows[1] != len(control_pre)):\n",
    "            print('the arg holdout_windows does not add up to the number of time units!')\n",
    "            print('holdout_windows = {0}'.format(holdout_windows))\n",
    "            print('total number of time periods = {0}'.format(len(control_pre)))\n",
    "        else:\n",
    "            pass    \n",
    "        ## Define the holdout samples\n",
    "        control_holdout = control_pre[0:holdout_windows[0]].copy()\n",
    "        treatment_holdout = treatment_pre[0:holdout_windows[0]].copy()    \n",
    "        \n",
    "        control_nonholdout = control_pre[holdout_windows[0]:].copy()\n",
    "        treatment_nonholdout = treatment_pre[holdout_windows[0]:].copy()\n",
    "        \n",
    "        ## Estimate the CL model\n",
    "        ## Let's loop over different treatment units.\n",
    "        holdout_dict = {}\n",
    "        holdout_dict['omega'] = []\n",
    "        holdout_dict['weights'] = []\n",
    "        diff_holdout_mse = []\n",
    "        diff_nonholdout_mse = []\n",
    "        if treatment_pre.shape[1] > 1:\n",
    "            for t in treatment_pre.columns:\n",
    "                t_dict = adh.get_w(control_holdout,treatment_holdout[t])\n",
    "                ## Estimate measure of fit for the hold out and non-holdout sample\n",
    "                diff_h = treatment_holdout[t]       - np.dot(control_holdout, t_dict.T)\n",
    "                diff_h_mse = (diff_h**2).mean()\n",
    "                diff_nh = treatment_nonholdout[t] - np.dot(control_nonholdout, t_dict.T)\n",
    "                diff_nh_mse = (diff_nh**2).mean()\n",
    "        \n",
    "                holdout_dict['omega'].append(t_dict)\n",
    "                holdout_dict['weights'].append(t_dict)\n",
    "                diff_holdout_mse.append(diff_h_mse)\n",
    "                diff_nonholdout_mse.append(diff_nh_mse)\n",
    "        else:\n",
    "            t_dict = adh.get_w(control_holdout,treatment_holdout.values.flatten())\n",
    "            ## Estimate measure of fit for the hold out and non-holdout sample\n",
    "            diff_h= treatment_holdout       - np.dot(control_holdout, np.array([t_dict]).T)\n",
    "            diff_h_mse = (diff_h**2).mean()\n",
    "            diff_nh = treatment_nonholdout - np.dot(control_nonholdout, np.array([t_dict]).T)\n",
    "            diff_nh_mse = (diff_nh**2).mean()\n",
    "\n",
    "            holdout_dict['omega'].append(t_dict)\n",
    "            holdout_dict['weights'].append(t_dict)\n",
    "            diff_holdout_mse.append(diff_h_mse)\n",
    "            diff_nonholdout_mse.append(diff_nh_mse)            \n",
    "        holdout_dict['omega'] = np.array(holdout_dict['omega'])\n",
    "        holdout_dict['mse_holdout'] =np.mean(diff_holdout_mse)\n",
    "        holdout_dict['mse_nonholdout'] =np.mean(diff_holdout_mse)        \n",
    "        \n",
    "        return holdout_dict    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5f16606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conformal Inference to do inference for the SC models\n",
    "import itertools\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng()\n",
    "rng.choice(10, size=10, replace=False)\n",
    "\n",
    "\n",
    "class conformal_inf:\n",
    "    def time_block_permutation(data=None,\n",
    "                              time_unit='date',\n",
    "                              post='W'):\n",
    "        tw =  len(data.loc[ data[post]==1][time_unit].unique())\n",
    "        treatment_window = len(data.loc[(data[post]==0)][time_unit].unique())-tw, tw\n",
    "\n",
    "        time_list = np.arange( np.sum(treatment_window) )\n",
    "        T_len = len(time_list)\n",
    "\n",
    "        ## Time block permutations\n",
    "        permutations_subset_block = []\n",
    "\n",
    "        for i in range(T_len):\n",
    "            half_A = time_list[-1*(T_len-i):]\n",
    "            half_B = time_list[0:i]\n",
    "            scrambled_list = np.concatenate([half_A, half_B]) \n",
    "            permutations_subset_block.append( list(scrambled_list)  )\n",
    "\n",
    "        return permutations_subset_block, treatment_window\n",
    "\n",
    "\n",
    "    def scrambled_residual(counterfactual, actual, \n",
    "                       scrambled_order,\n",
    "                      treatment_window):\n",
    "        '''\n",
    "        counterfactual   array of counterfactual estimates that are assumed to be ordered sequentially\n",
    "        actual           ``'' for actual values\n",
    "        scrambled_order  integer array that tells me how to scramble these\n",
    "        treatment_window list of two integers for the number of pre-treatment and post-treatment units\n",
    "        '''\n",
    "        counterfactual_ = counterfactual[scrambled_order].copy()        \n",
    "        actual_         = actual[scrambled_order].copy()\n",
    "        return np.abs(actual_ - counterfactual_)[ -1*treatment_window[1]:]\n",
    "    \n",
    "    def test_statS(q, treatment_window, residual_abs):\n",
    "        normed = np.sum(  np.power(residual_abs, q) )\n",
    "        return np.power( treatment_window[1]**(-0.5)*normed , 1/q)    \n",
    "\n",
    "    def pvalue_calc(counterfactual=None,\n",
    "                    actual=None, \n",
    "                    permutation_list=None,\n",
    "                   treatment_window=None,\n",
    "                   h0 = 0):\n",
    "#         print(treatment_window)\n",
    "        control_pst = counterfactual[-1*treatment_window[1]:].copy()\n",
    "        actual_pst  = actual[-1*treatment_window[1]:].copy()\n",
    "        actual_pst -= h0\n",
    "\n",
    "        ## Calculate the residual\n",
    "        residual_initial = np.abs(actual_pst - control_pst)         \n",
    "        S_q = conformal_inf.test_statS(1, treatment_window, residual_initial)\n",
    "#         print(residual_initial)\n",
    "        ## Now do a whole bunch of treatment time scrambles\n",
    "        ## We're going to permute over all time-based permutations\n",
    "        ## Adjust the actual by the null hypothesis \n",
    "        treat_ = actual.copy()\n",
    "        treat_[-1*treatment_window[1]:] -= h0\n",
    "        full_residual = np.abs(treat_ - counterfactual)\n",
    "        S_q_pi = []\n",
    "        for r,r_index in zip(permutation_list, range(len(permutation_list))):\n",
    "            scrambled_dates = np.array(list(r))              \n",
    "            residual_ = full_residual[scrambled_dates][-1*treatment_window[1]:].copy()\n",
    "#             if r_index==0:\n",
    "#                 print(residual_)            \n",
    "            S_q_pi.append(  conformal_inf.test_statS(1, treatment_window, residual_ )  )\n",
    "            \n",
    "        p_value = 1 - np.average( (np.array(S_q_pi) < S_q ) )\n",
    "        return p_value\n",
    "    \n",
    "    def ci_calc(y_hat=None,\n",
    "               y_act=None,\n",
    "               theta_grid=None,\n",
    "                permutation_list_ci = None,\n",
    "                treatment_window_ci =None,\n",
    "               alpha=0.05):\n",
    "        pv_grid = []\n",
    "        for t in theta_grid:\n",
    "#             print(t)\n",
    "            pv = conformal_inf.pvalue_calc(counterfactual=y_hat.copy(),\n",
    "                        actual=y_act.copy(), \n",
    "                        permutation_list = permutation_list_ci,\n",
    "                        treatment_window = treatment_window_ci,\n",
    "                        h0=t)\n",
    "            pv_grid.append(pv)   \n",
    "        ci_list = [ theta_grid[i] for i in range(len(pv_grid)) if pv_grid[i] > alpha ]\n",
    "#         print('\\nxxxxxxxx')\n",
    "#         for t, p in zip(theta_grid, pv_grid):\n",
    "#             print('{0:5.3f}  {1:5.3f}'.format(t,p))\n",
    "#         print([np.min(ci_list), np.max(ci_list)])\n",
    "        return {'theta_list':theta_grid, 'pvalue_list':pv_grid, 'ci_list':ci_list,\n",
    "               'ci_interval':[np.min(ci_list), np.max(ci_list)]}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83611126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b3391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3670939a",
   "metadata": {},
   "source": [
    "Test these functions out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a615d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "N = 50\n",
    "initial = np.random.uniform(0,2, N)\n",
    "df = pd.DataFrame(data={'y':initial,\n",
    "                  'unit_id':np.arange(N),\n",
    "                       'time':np.zeros(N).astype(int)})\n",
    "for t in range(10):\n",
    "    entry = df.iloc[-1*N:]\n",
    "    entry['y'] = entry['y']*0.80 + np.random.normal(0,1,N)*0.20\n",
    "    entry.loc[[True]*N,'time'] = t\n",
    "    df = pd.concat([df,entry])\n",
    "df['treated']        = df['unit_id'].isin([0,1])\n",
    "df['post'] = (df['time'] > 8)\n",
    "df['W'] =     df['treated']*    df['post']\n",
    "df['unit_id'] = df['unit_id'].apply(str)\n",
    "df.loc[df['W']==True, 'y'] += 5\n",
    "df.drop_duplicates(subset=['unit_id','time','treated'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca5a8a",
   "metadata": {},
   "source": [
    "SDiD Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "50c82154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sdid', 'sdid_model', 'omega_weights', 'lambda_weights', 'counterfactual'])\n"
     ]
    }
   ],
   "source": [
    "sdid_results = sdid.twfe_sdid(data=df,\n",
    "         data_dict={'treatment':'treated',\n",
    "                   'date':'time',\n",
    "                   'post':'post',\n",
    "                   'unitid':'unit_id',\n",
    "                   'outcome':'y'})\n",
    "print(sdid_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cd062069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  coef_           se_    pvalue\n",
      "const     -50771.953928  4.837117e+09  0.999992\n",
      "post_SDiD      5.093876  2.568556e-02  0.000000\n",
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.996\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.996\n",
      "Method:                 Least Squares   F-statistic:                              2099.\n",
      "Date:                Sun, 15 Jan 2023   Prob (F-statistic):                        0.00\n",
      "Time:                        14:50:11   Log-Likelihood:                          1329.4\n",
      "No. Observations:                 500   AIC:                                     -2545.\n",
      "Df Residuals:                     443   BIC:                                     -2305.\n",
      "Df Model:                          57                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const      -5.077e+04   4.84e+09  -1.05e-05      1.000   -9.51e+09    9.51e+09\n",
      "post_SDiD      5.0939      0.026    198.317      0.000       5.043       5.144\n",
      "1           1.039e+05   2.61e+10   3.98e-06      1.000   -5.13e+10    5.13e+10\n",
      "2          -6.288e+04   2.08e+10  -3.03e-06      1.000   -4.08e+10    4.08e+10\n",
      "3          -1.567e+05   1.66e+10  -9.43e-06      1.000   -3.26e+10    3.26e+10\n",
      "4          -2.425e+04   1.36e+10  -1.79e-06      1.000   -2.67e+10    2.67e+10\n",
      "5          -6.243e+04   1.46e+10  -4.29e-06      1.000   -2.86e+10    2.86e+10\n",
      "6          -6948.5723   1.07e+10  -6.47e-07      1.000   -2.11e+10    2.11e+10\n",
      "7           5.077e+04   4.84e+09   1.05e-05      1.000   -9.51e+09    9.51e+09\n",
      "8           5.691e+04   1.16e+10   4.89e-06      1.000   -2.29e+10    2.29e+10\n",
      "9           5.077e+04   4.84e+09   1.05e-05      1.000   -9.51e+09    9.51e+09\n",
      "11            -0.1220      0.030     -4.125      0.000      -0.180      -0.064\n",
      "12            -0.2262      0.030     -7.645      0.000      -0.284      -0.168\n",
      "13             0.0662      0.030      2.238      0.026       0.008       0.124\n",
      "14             0.2189      0.030      7.401      0.000       0.161       0.277\n",
      "15            -0.1573      0.030     -5.317      0.000      -0.215      -0.099\n",
      "16             0.0751      0.030      2.538      0.011       0.017       0.133\n",
      "17            -0.0405      0.030     -1.369      0.172      -0.099       0.018\n",
      "18             0.0319      0.030      1.078      0.281      -0.026       0.090\n",
      "19            -0.3308      0.030    -11.181      0.000      -0.389      -0.273\n",
      "2             -0.2537      0.030     -8.575      0.000      -0.312      -0.196\n",
      "20             0.3297      0.030     11.146      0.000       0.272       0.388\n",
      "21             0.0707      0.030      2.389      0.017       0.013       0.129\n",
      "22             0.3281      0.030     11.091      0.000       0.270       0.386\n",
      "23             0.0783      0.030      2.646      0.008       0.020       0.136\n",
      "24             0.0960      0.030      3.245      0.001       0.038       0.154\n",
      "25            -0.1908      0.030     -6.448      0.000      -0.249      -0.133\n",
      "26             0.3536      0.030     11.953      0.000       0.295       0.412\n",
      "27            -0.0671      0.030     -2.267      0.024      -0.125      -0.009\n",
      "28            -0.0224      0.030     -0.756      0.450      -0.081       0.036\n",
      "29             0.2817      0.030      9.521      0.000       0.224       0.340\n",
      "3              0.3604      0.030     12.183      0.000       0.302       0.419\n",
      "30            -0.0375      0.030     -1.267      0.206      -0.096       0.021\n",
      "31            -0.3598      0.030    -12.162      0.000      -0.418      -0.302\n",
      "32            -0.1049      0.030     -3.546      0.000      -0.163      -0.047\n",
      "33             0.2201      0.030      7.439      0.000       0.162       0.278\n",
      "34            -0.5463      0.030    -18.467      0.000      -0.604      -0.488\n",
      "35            -0.1165      0.030     -3.939      0.000      -0.175      -0.058\n",
      "36             0.1680      0.030      5.680      0.000       0.110       0.226\n",
      "37             0.3170      0.030     10.714      0.000       0.259       0.375\n",
      "38             0.1842      0.030      6.227      0.000       0.126       0.242\n",
      "39             0.3090      0.030     10.443      0.000       0.251       0.367\n",
      "4              0.1079      0.030      3.647      0.000       0.050       0.166\n",
      "40            -0.0619      0.030     -2.094      0.037      -0.120      -0.004\n",
      "41            -0.0754      0.030     -2.548      0.011      -0.134      -0.017\n",
      "42            -0.1855      0.030     -6.269      0.000      -0.244      -0.127\n",
      "43            -0.0132      0.030     -0.446      0.656      -0.071       0.045\n",
      "44            -0.3291      0.030    -11.126      0.000      -0.387      -0.271\n",
      "45            -0.0139      0.030     -0.468      0.640      -0.072       0.044\n",
      "46             0.2158      0.030      7.293      0.000       0.158       0.274\n",
      "47             0.2278      0.030      7.699      0.000       0.170       0.286\n",
      "48            -0.1645      0.030     -5.561      0.000      -0.223      -0.106\n",
      "49            -0.1146      0.030     -3.875      0.000      -0.173      -0.056\n",
      "5              0.0665      0.030      2.248      0.025       0.008       0.125\n",
      "6              0.1301      0.030      4.396      0.000       0.072       0.188\n",
      "7              0.0479      0.030      1.618      0.106      -0.010       0.106\n",
      "8             -0.0353      0.030     -1.192      0.234      -0.093       0.023\n",
      "9              0.1117      0.030      3.774      0.000       0.054       0.170\n",
      "==============================================================================\n",
      "Omnibus:                      164.720   Durbin-Watson:                   1.832\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            10428.574\n",
      "Skew:                           0.493   Prob(JB):                         0.00\n",
      "Kurtosis:                      25.352   Cond. No.                     2.03e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[3] The smallest eigenvalue is 1.69e-31. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "print(sdid_results['sdid'])\n",
    "print(sdid_results['sdid_model'].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874b206",
   "metadata": {},
   "source": [
    "DiD Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dd52ab03",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m did_twfe_results \u001b[38;5;241m=\u001b[39m \u001b[43mdid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtwfe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtreatment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtreated\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munitid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munit_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutcome\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[142], line 177\u001b[0m, in \u001b[0;36mdid.twfe\u001b[0;34m(data, covariates, data_dict)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     FJointPValue \u001b[38;5;241m=\u001b[39m event_model\u001b[38;5;241m.\u001b[39mf_test(A)\u001b[38;5;241m.\u001b[39mpvalue\u001b[38;5;241m.\u001b[39mitem()            \n\u001b[1;32m    169\u001b[0m event_pre_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_event\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_period\u001b[39m\u001b[38;5;124m'\u001b[39m:[x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m event_pre_coef\u001b[38;5;241m.\u001b[39mindex],\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreated_unit\u001b[39m\u001b[38;5;124m'\u001b[39m:[x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m event_pre_coef\u001b[38;5;241m.\u001b[39mindex],\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoef_\u001b[39m\u001b[38;5;124m'\u001b[39m:event_pre_coef,\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mse_\u001b[39m\u001b[38;5;124m'\u001b[39m:event_pre_se,\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtstat\u001b[39m\u001b[38;5;124m'\u001b[39m:event_pre_tstat,\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpvalue\u001b[39m\u001b[38;5;124m'\u001b[39m:event_pre_pvalues,\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFJointStat\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mevent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m(),\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFJointPValue\u001b[39m\u001b[38;5;124m'\u001b[39m:event_model\u001b[38;5;241m.\u001b[39mf_test(A)\u001b[38;5;241m.\u001b[39mpvalue\n\u001b[1;32m    179\u001b[0m                       })\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m## Test whether all pre-trend estimates are different from zero\u001b[39;00m\n\u001b[1;32m    183\u001b[0m A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;28mlen\u001b[39m(event_model\u001b[38;5;241m.\u001b[39mparams))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "did_twfe_results = did.twfe(data=df,\n",
    "                           data_dict={'treatment':'treated',\n",
    "                                     'date':'time',\n",
    "                                     'post':'post',\n",
    "                                     'unitid':'unit_id',\n",
    "                                     'outcome':'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot_ =did_twfe_results['twfe_c'].loc[did_twfe_results['twfe_c']['unit_id'].isin(['0','1'])]\n",
    "data_plot_.drop_duplicates(inplace=True)\n",
    "\n",
    "for r in ['0','1']:\n",
    "    fig,ax = plt.subplots(ncols=1,nrows=1,figsize=(12,5))\n",
    "    ax.plot(data_plot_.loc[data_plot_['unit_id']==r]['time'],\n",
    "            data_plot_.loc[data_plot_['unit_id']==r]['y_hat_counterfactual'], \n",
    "            marker='o', linestyle='--',  mfc='none', \n",
    "            color='black',\n",
    "            label='Predicted for Unit {0}'.format(r))\n",
    "    ax.plot(data_plot_.loc[data_plot_['unit_id']==r]['time'],\n",
    "            data_plot_.loc[data_plot_['unit_id']==r]['y'], \n",
    "            marker='o', linestyle='solid',  mfc='none', \n",
    "            color='green',\n",
    "            label='Actual for Unit {0}'.format(r))\n",
    "    ax.vlines(x=data_plot_.loc[(data_plot_['unit_id']==r) & (data_plot_['time'].isin([9])) ]['time'],\n",
    "           ymin=data_plot_.loc[(data_plot_['unit_id']==r) & (data_plot_['time'].isin([9])) ]['y']-1.96*\\\n",
    "            data_plot_.loc[(data_plot_['unit_id']==r) & (data_plot_['time'].isin([9])) ]['y_hat_se'],\n",
    "           ymax=data_plot_.loc[(data_plot_['unit_id']==r) & (data_plot_['time'].isin([9])) ]['y']+1.96*\\\n",
    "            data_plot_.loc[(data_plot_['unit_id']==r) & (data_plot_['time'].isin([9])) ]['y_hat_se'],\n",
    "              color='orange',\n",
    "             label='95% CI of ATET for Unit {0}'.format(r))\n",
    "    ax.set_xticks(data_plot_.loc[data_plot_['unit_id']=='0']['time'])\n",
    "    ax.set_xticklabels(data_plot_.loc[data_plot_['unit_id']=='0']['time'])\n",
    "    ax.set_ylabel('Outcome of Interest')\n",
    "    ax.set_xlabel('Time Horizon')\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ff016",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_placebo_plot_ = did_twfe_results['event_study'].copy()\n",
    "for r in ['0','1']:\n",
    "    fig,ax=plt.subplots(ncols=1,nrows=1,figsize=(10,3))\n",
    "    ax.scatter(event_placebo_plot_.loc[event_placebo_plot_['treated_unit']==r]['time_period'].astype(int),\n",
    "        event_placebo_plot_.loc[event_placebo_plot_['treated_unit']==r]['coef_'],\n",
    "        marker='o', edgecolor='green', label='Estimate with 95% CI for Unit {0}'.format(r))\n",
    "    ax.vlines(x=event_placebo_plot_.loc[event_placebo_plot_['treated_unit']==r]['time_period'].astype(int),\n",
    "            ymin=event_placebo_plot_.loc[event_placebo_plot_['treated_unit']==r]['coef_']-1.96*\\\n",
    "              event_placebo_plot_.loc[event_placebo_plot_['treated_unit']==r]['se_'],\n",
    "            ymax=event_placebo_plot_.loc[event_placebo_plot_['treated_unit']==r]['coef_']+1.96*\\\n",
    "              event_placebo_plot_.loc[event_placebo_plot_['treated_unit']==r]['se_'],          \n",
    "            linestyle='solid',  color='green')\n",
    "    ax.set_xticks(event_placebo_plot_.loc[event_placebo_plot_['treated_unit']==r]['time_period'].astype(int))\n",
    "    ax.set_xticklabels(event_placebo_plot_.loc[event_placebo_plot_['treated_unit']==r]['time_period'])\n",
    "    ax.set_ylabel('Event Study Estimate')\n",
    "    ax.set_xlabel('Time Horizon')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d2090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display( \n",
    "    did_twfe_results['twfe_c'].loc[did_twfe_results['twfe_c']['unit_id'].isin(['0','1'])]\n",
    ")\n",
    "display( \n",
    "    did_twfe_results['event_study_c'].loc[did_twfe_results['event_study_c']['unit_id'].isin(['0','1'])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6faa0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19de87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "048cd8af",
   "metadata": {},
   "source": [
    "SC Model Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc705b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean data\n",
    "sc_dict = dgp.clean_and_input_data(dataset=df,\n",
    "                                   treatment='treated',\n",
    "                                   unit_id='unit_id',\n",
    "                                   date='time',\n",
    "                                   post='post',\n",
    "                                  outcome='y')\n",
    "\n",
    "# ## Figure out the alpha and lambda values\n",
    "# w=alpha_lambda.get_alpha_lambda(sc_dict['C_pre'])\n",
    "# alpha_lambda_to_use = alpha_lambda.alpha_lambda_transform(w.x)\n",
    "# ## Take the alpha and lambda values, and estimate mu and omega\n",
    "# di_est = di.predict_mu_omega(sc_dict['T_pre'], sc_dict['C_pre'], alpha_lambda_to_use, \n",
    "#                              treatment_window_pre_treatment)\n",
    "# di_output = di.sc_style_results(sc_dict['T_pre'], sc_dict['T_pst'],\n",
    "#                     sc_dict['C_pre'], sc_dict['C_pst'],\n",
    "#                         di_est['mu'],di_est['omega'])\n",
    "# di_validation = sc.sc_validation(treatment_pre=sc_dict['T_pre'], \n",
    "#                  treatment_pst=sc_dict['T_pst'],\n",
    "#                  control_pre=sc_dict['C_pre'],\n",
    "#                  control_pst=sc_dict['C_pst'], \n",
    "#                  mu=di_est['mu'],\n",
    "#                  omega=di_est['omega'],\n",
    "#                  pre_treatment_window=treatment_window_pre_treatment)\n",
    "# cl_output['predict_est']\n",
    "\n",
    "# ak7 = cl.predict_mu_omega(sc_dict['T_pre'], sc_dict['C_pre'], treatment_window_pre_treatment)\n",
    "# cl_output = di.sc_style_results(sc_dict['T_pre'], sc_dict['T_pst'],\n",
    "#                     sc_dict['C_pre'], sc_dict['C_pst'],\n",
    "#                     ak7['mu'], ak7['omega'])\n",
    "# sc_validation = sc.sc_validation(treatment_pre=sc_dict['T_pre'], \n",
    "#                  treatment_pst=sc_dict['T_pst'],\n",
    "#                  control_pre=sc_dict['C_pre'],\n",
    "#                  control_pst=sc_dict['C_pst'], \n",
    "#                  mu=ak7['mu'],\n",
    "#                  omega=ak7['omega'],\n",
    "#                  pre_treatment_window=treatment_window_pre_treatment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = sc.sc_model(model_name='adh',\n",
    "        data=df,\n",
    "        data_dict={'treatment': 'treated',\n",
    "                  'date':'time',\n",
    "                  'post':'post',\n",
    "                  'unitid':'unit_id',\n",
    "                  'outcome':'y'},\n",
    "        pre_process_data=None,\n",
    "                 aggregate_pst_periods=True,\n",
    "        pre_treatment_window=None,\n",
    "        inference={'alpha':0.05, 'theta_grid':np.arange(-2,8,0.0151)})\n",
    "display( f3['results_df'] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f81d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.sc_generate_figures(final_sc_output=f3,\n",
    "                           output_figure_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e116c809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
