{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243a43a9",
   "metadata": {},
   "source": [
    "# panelib\n",
    "\n",
    "Julian Hsu\n",
    "This is our package for panel models. We use our own written synthetic control and a diff-in-diff model using `statsmodels`.\n",
    "\n",
    "**todolist**\n",
    "1. [COMPLETED: 14DEC2022] Correct p-value calculation for SC models \n",
    "2. [COMPLETED: 15DEC2022] Output aggregate ATET and p-value for SC models\n",
    "3. [COMPLETED: 15DEC2022] Write a function to plot SC model output\n",
    "4. Code up DiD Model\n",
    "    1. [COMPLETED: 16DEC2022] OLS model for TWFE\n",
    "    2. [COMPLETED: 16DEC2022] placebo OLS model with event study\n",
    "    3. Output F-test, t-tests, and graphical evidence\n",
    "    4. Output predicted counterfactual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "348a8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from IPython.display import display    \n",
    "\n",
    "import scipy.stats \n",
    "from sklearn.linear_model import ElasticNet\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from typing import List\n",
    "from operator import add\n",
    "from toolz import reduce, partial\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "028326b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "class did:\n",
    "    def treated_counterfactual(did_model=None,\n",
    "                            atet=None,\n",
    "                               df = None,\n",
    "                              data=None,\n",
    "                              data_dict={'treatment':None,\n",
    "                                      'date':None,\n",
    "                                      'post':None,\n",
    "                                      'unitid':None,\n",
    "                                      'outcome':None}):\n",
    "        '''\n",
    "        Estimate the counterfactual trend of the units if they were in control.\n",
    "        We do this by using the treated OLS model.\n",
    "        '''\n",
    "        df_c = df.copy()\n",
    "        df_c['y_hats'] = did_model.predict(data)\n",
    "\n",
    "        '''\n",
    "        We simply subtract the estimated ATET from the treated units.\n",
    "        To accommodate ATET that is a constant for each unit, and varies across units, we merge on ATET estimates.\n",
    "        '''\n",
    "        if 'time_period' in atet.columns:\n",
    "#             print(df_c[ [data_dict['unitid'], data_dict['date'] ]].dtypes)\n",
    "#             print(atet[ [ 'treated_unit', 'time_period']].dtypes)\n",
    "            df_c = df_c.merge(atet, left_on=[data_dict['unitid'], data_dict['date']],\n",
    "                              right_on = [ 'treated_unit', 'time_period'], how='left' )\n",
    "            df_c.fillna(0, inplace=True)\n",
    "            df_c['y_hat_counterfactual']   = df_c['y_hats'] -\\\n",
    "                    df_c['coef_']\n",
    "        else:\n",
    "            df_c = df_c.merge(atet, left_on=[data_dict['unitid']],\n",
    "                              right_on=['treated_unit'], how='left' )\n",
    "            df_c.fillna(0, inplace=True)\n",
    "            df_c['y_hat_counterfactual']   = df_c['y_hats'] -\\\n",
    "                        df_c['coef_']*(df_c[data_dict['post']]==1)\n",
    "\n",
    "        return df_c[[data_dict['unitid'], data_dict['date'], \n",
    "                     data_dict['outcome'],\n",
    "                     'y_hats','y_hat_counterfactual']]\n",
    "    \n",
    "    def twfe(data=None,\n",
    "            data_dict={'treatment':None,\n",
    "                      'date':None,\n",
    "                      'post':None,\n",
    "                      'unitid':None,\n",
    "                      'outcome':None}):\n",
    "        ## Construct the TWFE regression by creating time indicators and unit indicators\n",
    "        ## This estimate treatment-unit specific treatment effects\n",
    "        t_fe = pd.get_dummies(data[data_dict['date']])\n",
    "        x_fe = pd.get_dummies(data[data_dict['unitid']])\n",
    "\n",
    "        treated_units = data.loc[data[data_dict['treatment']]==1][data_dict['unitid']].unique().tolist()\n",
    "\n",
    "        for i,r in zip(range(len(treated_units)),  treated_units):\n",
    "            if i==0:\n",
    "                post_treated = pd.DataFrame(\n",
    "                                           data={'post_x_{0}'.format(r): \n",
    "                                                (data[data_dict['post']]*(data[data_dict['unitid']]==r)).astype(float)})\n",
    "            else:\n",
    "                post_treated['post_x_{0}'.format(r)]=(data[data_dict['post']]*(data[data_dict['unitid']]==r)).astype(float)\n",
    "\n",
    "        twfe_X = sm.add_constant( pd.concat([post_treated,  t_fe, x_fe ], axis=1)  )\n",
    "        twfe_model = sm.OLS(data[data_dict['outcome']],  twfe_X).fit()\n",
    "        twfe_coef = twfe_model.params.iloc[1:1+len(treated_units)]\n",
    "        twfe_se = twfe_model.bse.iloc[1:1+len(treated_units)]\n",
    "        twfe_tstat = twfe_model.tvalues.iloc[1:1+len(treated_units)]\n",
    "        twfe_pvalues = twfe_model.pvalues.iloc[1:1+len(treated_units)]        \n",
    "        '''\n",
    "        Output a dataframe that tells us the ATET by unit id and (if applicable) time period\n",
    "        '''\n",
    "\n",
    "        \n",
    "        df_twfe = pd.DataFrame()\n",
    "        for r, coef_, se_, pv_ in zip(twfe_coef.index, twfe_coef, twfe_se, twfe_pvalues):\n",
    "            unitid = r.split('_')[-1]\n",
    "            df_twfe = pd.concat([df_twfe,\n",
    "                                     pd.DataFrame(index=[r],\n",
    "                                                 data={'treated_unit':unitid,\n",
    "                                                      'coef_':coef_,\n",
    "                                                      'se_':se_,\n",
    "                                                      'pvalue':pv_}  )])        \n",
    "        df_c = did.treated_counterfactual(did_model=twfe_model,\n",
    "                                        atet=df_twfe,\n",
    "                                        df = data,\n",
    "                                        data = twfe_X,\n",
    "                                        data_dict = data_dict)\n",
    "\n",
    "        '''\n",
    "        Also construct the event-study approach\n",
    "        '''\n",
    "\n",
    "        ## Use the time period right before treatment as the hold out set\n",
    "        treated_units = data.loc[data[data_dict['treatment']]==1][data_dict['unitid']].unique().tolist()\n",
    "        hold_out_time = data.loc[ (data[data_dict['unitid']].isin(treated_units)) &\n",
    "                                  (data[data_dict['post']]==0)][data_dict['date']].max()\n",
    "        event_dummies = pd.DataFrame()\n",
    "        t_period_list = data.sort_values(by=data_dict['date'], ascending=True)[data_dict['date']].unique().tolist()\n",
    "        pre_treat_columns = []\n",
    "        pst_treat_columns = []\n",
    "        for t_period,i in zip(t_period_list, range(len(t_period_list))):\n",
    "            if t_period < hold_out_time:\n",
    "                for t_units_ in treated_units:\n",
    "                    event_dummies = pd.concat([event_dummies,\n",
    "                                              pd.DataFrame(data={\n",
    "                                                  'pre_treat_{0}_{1}'.format(i,t_units_):\n",
    "                                                  ( (data[data_dict['unitid']]==t_units_) *\n",
    "                                                  (data[data_dict['date']]==t_period)).astype(float)                                          \n",
    "                                              })],axis=1)\n",
    "                    pre_treat_columns.append('pre_treat_{0}_{1}'.format(i,t_units_))\n",
    "            elif t_period == hold_out_time:\n",
    "                pass\n",
    "            else:\n",
    "                for t_units_ in treated_units:\n",
    "                    event_dummies = pd.concat([event_dummies,\n",
    "                                              pd.DataFrame(data={\n",
    "                                                  'pst_treat_{0}_{1}'.format(i, t_units_):\n",
    "                                                  ( (data[data_dict['unitid']]==t_units_ )*\n",
    "                                                  (data[data_dict['date']]==t_period)).astype(float)                                          \n",
    "                                              })],axis=1)\n",
    "                    pst_treat_columns.append('pst_treat_{0}_{1}'.format(i,t_units_))\n",
    "        event_X = sm.add_constant( event_dummies  )\n",
    "        event_model = sm.OLS(data[data_dict['outcome']],  event_X).fit()\n",
    "        event_pre_coef = event_model.params[pre_treat_columns]\n",
    "        event_pre_se = event_model.bse[pre_treat_columns]\n",
    "        event_pre_tstat = event_model.tvalues[pre_treat_columns]\n",
    "        event_pre_pvalues = event_model.pvalues[pre_treat_columns]\n",
    "        \n",
    "        event_pst_coef = event_model.params[pst_treat_columns]\n",
    "        event_pst_se = event_model.bse[pst_treat_columns]\n",
    "        event_pst_tstat = event_model.tvalues[pst_treat_columns]\n",
    "        event_pst_pvalues = event_model.pvalues[pst_treat_columns]\n",
    "\n",
    "        \n",
    "\n",
    "        ## Test whether all pre-trend estimates are different from zero\n",
    "        A = np.identity(len(event_model.params))\n",
    "        A = A[1:,:]\n",
    "        for i,name in zip(range(len(event_model.params)), event_model.params.index):\n",
    "            if 'pre' in name:\n",
    "                A[:,i] = 0\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        event_pre_df = pd.DataFrame(data={\n",
    "            'pre_event':1,\n",
    "            'time_period':[x.split('_')[-2] for x in event_pre_coef.index],\n",
    "            'treated_unit':[x.split('_')[-1] for x in event_pre_coef.index],\n",
    "            'coef_':event_pre_coef,\n",
    "            'se_':event_pre_se,\n",
    "            'tstat':event_pre_tstat,\n",
    "            'pvalue':event_pre_pvalues,\n",
    "            'FJointStat':event_model.f_test(A).statistic,\n",
    "            'FJointPValue':event_model.f_test(A).pvalue\n",
    "                              })\n",
    "        \n",
    "\n",
    "        ## Test whether all pre-trend estimates are different from zero\n",
    "        A = np.identity(len(event_model.params))\n",
    "        A = A[1:,:]\n",
    "        for i,name in zip(range(len(event_model.params)), event_model.params.index):\n",
    "            if 'pst' in name:\n",
    "                A[:,i] = 0\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        event_pst_df = pd.DataFrame(data={\n",
    "            'pre_event':0,\n",
    "            'time_period':[ int(x.split('_')[-2]) for x in event_pst_coef.index],\n",
    "            'treated_unit':[ x.split('_')[-1] for x in event_pst_coef.index],\n",
    "            'coef_':event_pst_coef,\n",
    "            'se_':event_pst_se,\n",
    "            'tstat':event_pst_tstat,\n",
    "            'pvalue':event_pst_pvalues,\n",
    "            'FJointStat':event_model.f_test(A).statistic,\n",
    "            'FJointPValue':event_model.f_test(A).pvalue                   \n",
    "                              })\n",
    "\n",
    "        df_event_study_c = did.treated_counterfactual(did_model=event_model,\n",
    "                                        atet=event_pst_df.loc[event_pst_df.index.str.contains('pst')],\n",
    "                                        df = data,\n",
    "                                        data = event_X,\n",
    "                                        data_dict = data_dict)\n",
    "\n",
    "        \n",
    "        \n",
    "        return {'twfe':df_twfe, \n",
    "                'twfe_c': df_c,\n",
    "                'event_study':pd.concat([event_pre_df, event_pst_df]),\n",
    "                'event_study_c':df_event_study_c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1fa4340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the univeral function that calls all the different SC models coded up below.\n",
    "## It will also call functions used for SC model validation\n",
    "\n",
    "class sc:\n",
    "    def sc_model(model_name='adh',\n",
    "                data=None,\n",
    "                data_dict= {'treatment':None, \n",
    "                            'date':None, \n",
    "                            'post':None, \n",
    "                            'unitid':None, \n",
    "                            'outcome':None},\n",
    "                pre_process_data=None,\n",
    "                pre_treatment_window=None,\n",
    "                aggregate_pst_periods=True,\n",
    "                inference={'alpha':0.05,\n",
    "                          'theta_grid':np.arange(-10,10,0.005)}):\n",
    "        ## First pre-process the data if possible.\n",
    "        if pre_process_data==None:\n",
    "            pre_process_data = dgp.clean_and_input_data(dataset=data,\n",
    "                                                        treatment=data_dict['treatment'],\n",
    "                                                        unit_id=data_dict['unitid'],\n",
    "                                                        date=data_dict['date'],\n",
    "                                                        post=data_dict['post'],\n",
    "                                                        outcome=data_dict['outcome'])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        ## Second calculate the pre-treatment-window that can be useful for placebo tests\n",
    "        pre_treatment_window = dgp.determine_pre_treatment_window(ci_data_output=pre_process_data,\n",
    "                                                                 pre_treatment_window=pre_treatment_window)\n",
    "        \n",
    "        \n",
    "        ## Finally, call an SC model.\n",
    "        if model_name=='adh':\n",
    "            print('Using ADH')\n",
    "            sc_est = adh.predict_omega(pre_process_data['T_pre'], \n",
    "                                    pre_process_data['C_pre'], \n",
    "                                    pre_treatment_window)\n",
    "            sc_output = di.sc_style_results(pre_process_data['T_pre'], \n",
    "                                             pre_process_data['T_pst'],\n",
    "                                             pre_process_data['C_pre'], \n",
    "                                             pre_process_data['C_pst'],\n",
    "                                np.zeros(pre_process_data['T_pst'].shape[1]), np.array(sc_est['omega']))\n",
    "            sc_est['mu'] = np.zeros( pre_process_data['T_pre'].shape[1] )\n",
    "            \n",
    "        elif model_name=='di':\n",
    "            print('Using DI')\n",
    "            w=alpha_lambda.get_alpha_lambda(sc_dict['C_pre'])\n",
    "            alpha_lambda_to_use = alpha_lambda.alpha_lambda_transform(w.x)\n",
    "            ## Take the alpha and lambda values, and estimate mu and omega\n",
    "            sc_est = di.predict_mu_omega(pre_process_data['T_pre'], \n",
    "                                         pre_process_data['C_pre'], alpha_lambda_to_use, \n",
    "                                         pre_treatment_window)\n",
    "            sc_output = di.sc_style_results(pre_process_data['T_pre'], \n",
    "                                            pre_process_data['T_pst'],\n",
    "                                            pre_process_data['C_pre'], \n",
    "                                            pre_process_data['C_pst'],\n",
    "                                            sc_est['mu'],sc_est['omega'])\n",
    "            \n",
    "        elif model_name=='cl':\n",
    "            print('Using CL')\n",
    "            sc_est = cl.predict_mu_omega(pre_process_data['T_pre'],\n",
    "                                      pre_process_data['C_pre'], \n",
    "                                      pre_treatment_window)\n",
    "            sc_output = di.sc_style_results(pre_process_data['T_pre'],\n",
    "                                            pre_process_data['T_pst'],\n",
    "                                            pre_process_data['C_pre'], \n",
    "                                            pre_process_data['C_pst'],\n",
    "                                            sc_est['mu'], sc_est['omega']) \n",
    "        else:\n",
    "            print('SC Model name not supported. \\n [adh,di,cl] are supported models.')\n",
    "        \n",
    "        ## Output measures of fit pre-treatment (training), pre_treatment (test), and post-treatment\n",
    "        sc_validation = sc.sc_validation(treatment_pre=pre_process_data['T_pre'], \n",
    "                     treatment_pst=pre_process_data['T_pst'],\n",
    "                     control_pre=  pre_process_data['C_pre'],\n",
    "                     control_pst=  pre_process_data['C_pst'], \n",
    "                     mu=  sc_est['mu'],\n",
    "                     omega=sc_est['omega'],\n",
    "                     pre_treatment_window=pre_treatment_window)\n",
    "        \n",
    "        ## Improve on this by calling for inference results\n",
    "        sc_df_results = sc.collect_sc_outputs(sc_output = sc_output,\n",
    "                           pre_process_data=pre_process_data,\n",
    "                       theta_grid = inference['theta_grid'],\n",
    "                           pre_treatment_window=pre_treatment_window, \n",
    "                                            aggregate_pst_periods=aggregate_pst_periods,\n",
    "                                              alpha = inference['alpha'])\n",
    "        \n",
    "        return {**sc_output, 'results_df':sc_df_results}\n",
    "\n",
    "\n",
    "    '''\n",
    "    Create a function that evaluates how well the SC model does at matching the:\n",
    "    1. pre-treatment data used for training;\n",
    "    2. pre-treatment data used for testing; and\n",
    "    3. the post-treatment data.\n",
    "    \n",
    "    Calculate metrics of fit and compare pre-treatment and post-treatment metrics.\n",
    "    '''\n",
    "    def sc_validation(treatment_pre, treatment_pst, control_pre, control_pst, \n",
    "                                      mu,omega,\n",
    "                                     pre_treatment_window):\n",
    "        ## Re-combine the observed treatment and control outcomes\n",
    "        y_treat_obs = pd.concat([treatment_pre, treatment_pst], axis=0)\n",
    "        y_control_obs = pd.concat([control_pre, control_pst], axis=0)\n",
    "\n",
    "        ## Estimate the counterfactual outcome of the treatment group\n",
    "        y_treat_hat = mu + np.dot(y_control_obs, omega.T)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_percentage_error\n",
    "        def comparison_over_windows(x,y,\n",
    "                                    pre_treatment_window,\n",
    "                                   metric_func, index_name):\n",
    "            x_pre_train = x[0:pre_treatment_window[0]]\n",
    "            y_pre_train = y[0:pre_treatment_window[0]]\n",
    "            x_pre_test = x[pre_treatment_window[0]:pre_treatment_window[0]+pre_treatment_window[1]]\n",
    "            y_pre_test = y[pre_treatment_window[0]:pre_treatment_window[0]+pre_treatment_window[1]]\n",
    "            x_pst_test = x[-1*(pre_treatment_window[0]+pre_treatment_window[1]):].copy()\n",
    "            y_pst_test = y[-1*(pre_treatment_window[0]+pre_treatment_window[1]):].copy()\n",
    "            test_pre_train = metric_func(x_pre_train, y_pre_train)\n",
    "            test_pre_test  = metric_func(x_pre_test,  y_pre_test)\n",
    "            test_pst_test  = metric_func(x_pst_test,  y_pst_test)\n",
    "            \n",
    "            return pd.DataFrame(index=[index_name], data={'test_pre_train':test_pre_train,\n",
    "                                                          'test_pre_train_N':pre_treatment_window[0],\n",
    "                   'test_pre_test':test_pre_test,\n",
    "                   'test_pre_test_N':pre_treatment_window[1],\n",
    "                   'test_pst_test':test_pst_test,\n",
    "                    'test_pst_test_N':len(y_pst_test)})\n",
    "\n",
    "        ## Compare the predicted treatment with the observed treatment\n",
    "        treat_hat_treat_obs = comparison_over_windows(y_treat_hat, y_treat_obs,\n",
    "                                                   pre_treatment_window,\n",
    "                                                   mean_absolute_percentage_error,'mape_vs_treat_obs')\n",
    "        return treat_hat_treat_obs\n",
    "\n",
    "    def sc_validation_gather(counterfactual=None,\n",
    "                              actual=None,\n",
    "                             pre_treatment_window=None):\n",
    "        from sklearn.metrics import mean_absolute_percentage_error        \n",
    "\n",
    "        x_pre_train = counterfactual[0:pre_treatment_window[0]]\n",
    "        y_pre_train = actual[0:pre_treatment_window[0]]\n",
    "        x_pre_test = counterfactual[pre_treatment_window[0]:pre_treatment_window[0]+pre_treatment_window[1]]\n",
    "        y_pre_test = actual[pre_treatment_window[0]:pre_treatment_window[0]+pre_treatment_window[1]]\n",
    "        x_pst_test = counterfactual[-1*(pre_treatment_window[0]+pre_treatment_window[1]):].copy()\n",
    "        y_pst_test = actual[-1*(pre_treatment_window[0]+pre_treatment_window[1]):].copy()\n",
    "        test_pre_train = mean_absolute_percentage_error(x_pre_train, y_pre_train)\n",
    "        test_pre_test  = mean_absolute_percentage_error(x_pre_test,  y_pre_test)\n",
    "        test_pst_test  = mean_absolute_percentage_error(x_pst_test,  y_pst_test)\n",
    "            \n",
    "        return pd.DataFrame(index=['mape_test'], data={'test_pre_train':test_pre_train,\n",
    "               'test_pre_test':test_pre_test,\n",
    "               'test_pst_test':test_pst_test})\n",
    "\n",
    "    '''\n",
    "    Output a dataframe collecting different metrics for each treated unit,\n",
    "    such as the ATET, p-value, confidence interval, and placebo tests.\n",
    "    \n",
    "    All confidence intervals and p-values to be calculated for individual post-treatment periods\n",
    "    '''\n",
    "    def collect_sc_outputs(sc_output = None,\n",
    "                           pre_process_data=None,\n",
    "                           theta_grid = None,  # np.arange(-10,10,0.5),\n",
    "                           pre_treatment_window=None,\n",
    "                           aggregate_pst_periods=True,\n",
    "                      alpha = 0.05):\n",
    "        \n",
    "        if aggregate_pst_periods==True:\n",
    "            a = sc.collect_sc_outputs_aggregate(sc_output = sc_output,\n",
    "                           pre_process_data=pre_process_data,\n",
    "                           theta_grid = theta_grid,  # np.arange(-10,10,0.5),\n",
    "                           pre_treatment_window=pre_treatment_window,\n",
    "                      alpha = 0.05)\n",
    "        else:\n",
    "            a = sc.collect_sc_outputs_individual(sc_output = sc_output,\n",
    "               pre_process_data=pre_process_data,\n",
    "               theta_grid = theta_grid,  # np.arange(-10,10,0.5),\n",
    "               pre_treatment_window=pre_treatment_window,\n",
    "              alpha = 0.05)\n",
    "        return a\n",
    "    def collect_sc_outputs_individual(sc_output = None,\n",
    "                           pre_process_data=None,\n",
    "                           theta_grid = None,  # np.arange(-10,10,0.5),\n",
    "                           pre_treatment_window=None,\n",
    "                           aggregate_pst_periods=True,\n",
    "                      alpha = 0.05):\n",
    "        ## To do the individual results, just call the function that does that aggregated\n",
    "        ## results multiple times, assuming that there is only one post-treatment period.\n",
    "        \n",
    "        ## Calculate some primatives:\n",
    "        pre_T = pre_process_data['T_pre'].shape[0]\n",
    "        pst_T = pre_process_data['T_pst'].shape[0]\n",
    "        \n",
    "        \n",
    "        ## Create all permutations over all per-treatment periods and just one post-treatment period\n",
    "        permutations_subset_block_individual = []\n",
    "        individual_time_list = np.arange( pre_treatment_window[0]+1 )        \n",
    "        for i in range(pre_T+1):\n",
    "            half_A = individual_time_list[-1*(pre_T-i):].copy()\n",
    "            half_B = individual_time_list[0:i]\n",
    "            scrambled_list = np.concatenate([half_A, half_B]) \n",
    "            permutations_subset_block_individual.append( list(scrambled_list)  )\n",
    "        \n",
    "        ## Go through different permutations of pre- and post-periods to do inference\n",
    "        collect_individual_df = pd.DataFrame()\n",
    "        for t_pst in range(0, pst_T):\n",
    "#             print(pre_process_data['treatment_window'])\n",
    "            pst_index = np.append(np.arange(pre_T) ,[pre_T+t_pst]) \n",
    "\n",
    "            sc_output_subset = {'atet':sc_output['atet'].iloc[t_pst],\n",
    "                               'predict_est':sc_output['predict_est'].iloc[pst_index]}\n",
    "            pre_process_data_subset = {'time_scramble': permutations_subset_block_individual,\n",
    "                                      'treatment_window':(pre_process_data['treatment_window'][0]\n",
    "                                                         ,\n",
    "                                                         1)}\n",
    "            a = sc.collect_sc_outputs_aggregate(sc_output = sc_output_subset,\n",
    "                                       pre_process_data=pre_process_data_subset,\n",
    "                                       theta_grid = theta_grid,\n",
    "                                       pre_treatment_window=(pre_process_data['treatment_window'][0]\n",
    "                                                         ,\n",
    "                                                         1),\n",
    "                                  alpha = 0.05,\n",
    "                                               time_period_forindividual=t_pst)\n",
    "            a['individual_post']     = t_pst\n",
    "            collect_individual_df = pd.concat([collect_individual_df, a])\n",
    "        return collect_individual_df\n",
    "    \n",
    "    def collect_sc_outputs_aggregate(sc_output = None,\n",
    "                           pre_process_data=None,\n",
    "                           theta_grid = None,  # np.arange(-10,10,0.5),\n",
    "                           pre_treatment_window=None,\n",
    "                          alpha = 0.05,\n",
    "                                    time_period_forindividual=None): \n",
    "        ## Alter this function to collect information that is aggregated across all time periods,\n",
    "        ## and for information that is for a specific time period.\n",
    "        if len(sc_output['atet'].shape) > 1 :\n",
    "            sc_results = sc_output['atet'].mean(axis=0).copy().to_frame().rename(columns={0:'atet'})\n",
    "        else:\n",
    "            number_of_treated_units = len([x for x in sc_output['predict_est'].columns if '_est' not in x])  \n",
    "            sc_results = pd.DataFrame(index=[np.arange(number_of_treated_units)], \n",
    "                                      data={'time_period':time_period_forindividual,\n",
    "                                            'atet': sc_output['atet'].values })\n",
    "            \n",
    "        sc_pv = []\n",
    "        sc_ci_05 = []\n",
    "        sc_ci_95 = []\n",
    "        sc_se = []\n",
    "        sc_placebo_pre_train = []\n",
    "        sc_placebo_pre_test = []\n",
    "        sc_placebo_pst_test = []\n",
    "            \n",
    "        o = 0\n",
    "        for p in [x for x in sc_output['predict_est'].columns if '_est' not in x]:\n",
    "            ## Calculate the p-value\n",
    "            pv_output = conformal_inf.pvalue_calc(counterfactual=np.array( sc_output['predict_est']['{0}_est'.format(p)].tolist() ),\n",
    "                                      actual=np.array( sc_output['predict_est']['{0}'.format(p)].tolist() ),\n",
    "                                      permutation_list =pre_process_data['time_scramble'],\n",
    "                                      treatment_window = pre_process_data['treatment_window'],\n",
    "                                      h0=0)\n",
    "            sc_pv.append(pv_output)\n",
    "\n",
    "            ## Calculate the confidence interval\n",
    "            ## If no pre-defined theta grid is defined, then just look 100 values in either direction\n",
    "            ## of the ATET estimate\n",
    "            if theta_grid is None:\n",
    "                i = sc_results['atet'].iloc[o]/100\n",
    "                theta_grid_use = np.arange(-500*i+sc_results['atet'].iloc[o],\n",
    "                                      -500*i+sc_results['atet'].iloc[o], i*5 )            \n",
    "            else:\n",
    "                theta_grid_use = theta_grid.copy()\n",
    "            \n",
    "            ci_output = conformal_inf.ci_calc(y_hat=sc_output['predict_est']['{0}_est'.format(p)].values,\n",
    "                           y_act=sc_output['predict_est']['{0}'.format(p)].values,\n",
    "                           theta_grid=theta_grid_use,\n",
    "                           permutation_list_ci =pre_process_data['time_scramble'],\n",
    "                           treatment_window_ci = pre_process_data['treatment_window'],\n",
    "                           alpha=alpha)\n",
    "#             print( ci_output['theta_list'] )\n",
    "#             print( ci_output['pvalue_list'] )\n",
    "            sc_ci_05.append(ci_output['ci_interval'][0])\n",
    "            sc_ci_95.append(ci_output['ci_interval'][1])\n",
    "\n",
    "            ## Calculate the placebo tests for that treated unit too\n",
    "            placebo_df = sc.sc_validation_gather(counterfactual=np.array( sc_output['predict_est']['{0}_est'.format(p)].tolist() ),\n",
    "                                      actual=np.array( sc_output['predict_est']['{0}'.format(p)].tolist() ),\n",
    "                                     pre_treatment_window=pre_process_data['treatment_window'])\n",
    "            sc_placebo_pre_train.append(placebo_df['test_pre_train'].values[0])\n",
    "            sc_placebo_pre_test.append( placebo_df['test_pre_test'].values[0])\n",
    "            sc_placebo_pst_test.append( placebo_df['test_pst_test'].values[0]) \n",
    "            o+=1\n",
    "\n",
    "        sc_results['pvalues'] = sc_pv\n",
    "        sc_results['ci_lower'] = sc_ci_05\n",
    "        sc_results['ci_upper'] = sc_ci_95\n",
    "        sc_results['alpha'] = alpha\n",
    "        sc_results['test_pre_train_MAPE'] = sc_placebo_pre_train\n",
    "        sc_results['test_pre_test_MAPE'] = sc_placebo_pre_test\n",
    "        sc_results['test_pst_test_MAPE'] = sc_placebo_pst_test\n",
    "        \n",
    "        return sc_results\n",
    "\n",
    "\n",
    "    def sc_generate_figures(final_sc_output=None,\n",
    "                           output_figure_name=None):\n",
    "        def check_write_file(suffix=None):\n",
    "            if output_figure_name==None:\n",
    "                pass\n",
    "            else:\n",
    "                plt.savefig(output_figure_name+'_{0}.png'.format(suffix))\n",
    "\n",
    "        ## Single Picture\n",
    "        fig,ax = plt.subplots(ncols=1,nrows=1, figsize=(12,6))\n",
    "        for r in [o for o in final_sc_output['predict_est'].columns if '_est' not in o]:\n",
    "            c = ax.plot(final_sc_output['predict_est'].index,final_sc_output['predict_est'][r+'_est'],\n",
    "                        color=None,\n",
    "                        linestyle='--',\n",
    "                        label='Estimated Control for {0}'.format(r),\n",
    "                   marker='o', markerfacecolor='white',markeredgecolor=None)\n",
    "            t = ax.plot(final_sc_output['predict_est'].index,final_sc_output['predict_est'][r],\n",
    "                        color=c[0].get_color(),\n",
    "                        label='Observed for {0}'.format(r),\n",
    "                   marker='o', markerfacecolor=c[0].get_color(), markeredgecolor=c[0].get_color())\n",
    "            \n",
    "        xmin_, xmax_, ymin_, ymax_ = plt.axis()\n",
    "        ax.set_xlabel('Time Periods')\n",
    "        ax.set_ylabel('Outcome')\n",
    "        ax.vlines(x=final_sc_output['atet'].index.min(), \n",
    "                  ymin=ymin_, ymax=ymax_, \n",
    "                  color='black',\n",
    "                  linewidth=2,\n",
    "                 label='Treatment Time')\n",
    "        ax.grid()\n",
    "        plt.legend()\n",
    "        check_write_file(suffix='overall')\n",
    "        plt.show()\n",
    "\n",
    "        ## Picture for each Treated Unit\n",
    "        number_of_treated_units = len([o for o in final_sc_output['predict_est'].columns if '_est' not in o])\n",
    "        fig,ax = plt.subplots(ncols=1,nrows=number_of_treated_units, figsize=(12,6*number_of_treated_units))\n",
    "        for i,r in zip(range(number_of_treated_units),\n",
    "                       [o for o in final_sc_output['predict_est'].columns if '_est' not in o]):\n",
    "            c = ax[i].plot(final_sc_output['predict_est'].index,final_sc_output['predict_est'][r+'_est'],\n",
    "                        color=None,\n",
    "                        linestyle='--',\n",
    "                        label='Estimated Control for {0}'.format(r),\n",
    "                   marker='o', markerfacecolor='white',markeredgecolor=None)\n",
    "            t = ax[i].plot(final_sc_output['predict_est'].index,final_sc_output['predict_est'][r],\n",
    "                        color=c[0].get_color(),\n",
    "                        label='Observed for {0}'.format(r),\n",
    "                   marker='o', markerfacecolor=c[0].get_color(), markeredgecolor=c[0].get_color())\n",
    "            xmin_, xmax_, ymin_, ymax_ = ax[i].axis()\n",
    "            ax[i].set_xlabel('Time Periods')\n",
    "            ax[i].set_ylabel('Outcome')\n",
    "            ax[i].vlines(x=final_sc_output['atet'].index.min(), \n",
    "                      ymin=ymin_, ymax=ymax_, \n",
    "                      color='black',\n",
    "                      linewidth=2,\n",
    "                     label='Treatment Time')    \n",
    "            ax[i].grid()\n",
    "            ax[i].legend()\n",
    "            ax[i].set_title('Estimated and Observed Trends for {0}'.format(r))\n",
    "        check_write_file(suffix='deep_dive')        \n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "## DGP and functions to determine what types of SC models to do, and calculate primatives for SC models\n",
    "class dgp:    \n",
    "    def determine_pre_treatment_window(ci_data_output=None,\n",
    "                                      pre_treatment_window=None):\n",
    "        if pre_treatment_window ==None:\n",
    "            pre_treatment_len = ci_data_output['C_pre'].shape[0]\n",
    "            pre_t0 = int( 0.75*pre_treatment_len )\n",
    "            if pre_t0 < 1:\n",
    "                pre_t0=1\n",
    "            else:\n",
    "                pass            \n",
    "            pre_t1 = pre_treatment_len - pre_t0\n",
    "            pre_treatment_window = [pre_t0, pre_t1]\n",
    "        else:\n",
    "            pass\n",
    "        return pre_treatment_window \n",
    "    \n",
    "    def clean_and_input_data(dataset=None, \n",
    "                             treatment='treated_unit', \n",
    "                             unit_id = 'unitid',\n",
    "                             date='T',\n",
    "                             post='post', outcome='Y'):\n",
    "        \n",
    "        C_pre = dataset.loc[(dataset[treatment]==0) & (dataset[post]==0)].pivot_table(columns=unit_id,\n",
    "                                                index=date,\n",
    "                                                values=outcome)\n",
    "        C_pst = dataset.loc[(dataset[treatment]==0) & (dataset[post]==1)].pivot_table(columns=unit_id,\n",
    "                                                index=date,\n",
    "                                                values=outcome)\n",
    "        T_pre = dataset.loc[(dataset[treatment]==1) & (dataset[post]==0)].pivot_table(columns=unit_id,\n",
    "                                                index=date,\n",
    "                                                values=outcome)\n",
    "        T_pst = dataset.loc[(dataset[treatment]==1) & (dataset[post]==1)].pivot_table(columns=unit_id,\n",
    "                                                index=date,\n",
    "                                                values=outcome)\n",
    "        \n",
    "        permutations_subset_block = conformal_inf.time_block_permutation(data=dataset, \n",
    "                                                                         time_unit=date,\n",
    "                                                                         post=post)\n",
    "                \n",
    "        return {'C_pre':C_pre, 'C_pst':C_pst, 'T_pre':T_pre, 'T_pst':T_pst, \n",
    "                'time_scramble':permutations_subset_block[0],\n",
    "               'treatment_window':permutations_subset_block[1]}\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "6a44a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doudchenko Imbens, (2016) Model\n",
    "class di:\n",
    "    def estimate_mu_omega(treatment_pre, control_pre, alpha_lambda_0):\n",
    "        alpha_0, lambda_0 = alpha_lambda_0[0], alpha_lambda_0[1]\n",
    "        elnet = ElasticNet(random_state=2736, alpha=alpha_0, l1_ratio=lambda_0)\n",
    "        elnet.fit(control_pre, treatment_pre )\n",
    "        ## Output interpretable weights\n",
    "        try:\n",
    "            df_weights= pd.DataFrame(data=zip(treatment_pre.columns,\n",
    "                                              elnet.coef_.T\n",
    "                                             ))\n",
    "        except:\n",
    "            df_weights = pd.DataFrame(index=np.arange(len(elnet.coef_)), \n",
    "                         data=elnet.coef_.T)        \n",
    "        return {'mu': elnet.intercept_, 'omega': elnet.coef_, 'weights':df_weights, 'full':elnet}\n",
    "\n",
    "    def predict_mu_omega(treatment_pre, control_pre, alpha_lambda_0, holdout_windows):\n",
    "        ## Don't use all the control data\n",
    "        ## Make sure that the holdout windows add up to the total number of pre-treatment units\n",
    "        if (holdout_windows[0]+holdout_windows[1] != len(control_pre)):\n",
    "            print('the arg holdout_windows does not add up to the number of time units!')\n",
    "            print('holdout_windows = {0}'.format(holdout_windows))\n",
    "            print('total number of time periods = {0}'.format(len(control_pre)))\n",
    "        else:\n",
    "            pass    \n",
    "        ## Define the holdout samples\n",
    "        control_holdout = control_pre[0:holdout_windows[0]].copy()\n",
    "        treatment_holdout = treatment_pre[0:holdout_windows[0]] .copy()   \n",
    "        \n",
    "        control_nonholdout = control_pre[holdout_windows[0]:].copy()\n",
    "        treatment_nonholdout = treatment_pre[holdout_windows[0]:].copy()\n",
    "        \n",
    "        ## Estimate the DI model\n",
    "        holdout_dict = di.estimate_mu_omega(treatment_holdout, control_holdout, alpha_lambda_0)\n",
    "        if treatment_pre.shape[1]==1:\n",
    "            holdout_dict['omega'] = np.array([holdout_dict['omega']])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        ## Estimate measure of fit for the hold out and non-holdout sample\n",
    "        diff_holdout = treatment_holdout       -\\\n",
    "            np.dot(control_holdout, holdout_dict['omega'].T)+holdout_dict['mu']        \n",
    "        diff_nonholdout = treatment_nonholdout -\\\n",
    "            np.dot(control_nonholdout, holdout_dict['omega'].T)+holdout_dict['mu']\n",
    "\n",
    "        diff_nonholdout_mse = (diff_nonholdout**2).mean()\n",
    "        diff_holdout_mse = (diff_holdout**2).mean()\n",
    "        return {'mu':     holdout_dict['mu'],\n",
    "               'omega':   holdout_dict['omega'],\n",
    "               'weights': holdout_dict['weights'],\n",
    "               'full':    holdout_dict['full'],\n",
    "               'mse_holdout': diff_holdout_mse,\n",
    "               'mse_nonholdout':diff_nonholdout_mse}\n",
    "    \n",
    "    def sc_style_results(treatment_pre, treatment_pst, control_pre, control_pst, mu,omega):\n",
    "        ## Do some normalization of the omega input\n",
    "        if len(omega.shape) > 2:\n",
    "            omega = omega.reshape(omega.shape[-2],omega.shape[-1])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        final_X = pd.concat([treatment_pre, treatment_pst], axis=0)\n",
    "        control_X = pd.concat([control_pre, control_pst], axis=0)\n",
    "\n",
    "        control_df = mu + pd.DataFrame(data=np.dot(control_X, omega.T), columns=[ l+'_est' for l in final_X.columns ])\n",
    "        control_df.index = final_X.index\n",
    "        \n",
    "        output_df = control_df.join(final_X)\n",
    "        \n",
    "        treatment_periods = -1*len(treatment_pst)\n",
    "        atet_df = pd.DataFrame()\n",
    "        for c in [l for l in output_df.columns if '_est' not in l]:\n",
    "            diff = output_df[c][treatment_periods:].copy() - output_df[c+'_est'][treatment_periods:].copy()\n",
    "            atet_df[c] = diff\n",
    "        \n",
    "        return {'atet': atet_df , 'predict_est':output_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5ca4ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here code up the functions that we will try to minimize over\n",
    "class alpha_lambda:\n",
    "    def diff(y_t,y_c, mu_x, omega_x):\n",
    "        return y_t - mu_x - np.dot(y_c,omega_x)\n",
    "\n",
    "    def alpha_lambda_transform(alpha_lambda_raw_):\n",
    "        ## Alpha is strictly greater than zero\n",
    "        ## lambda exists between 0 and 1\n",
    "        return np.exp(alpha_lambda_raw_[0]/1000), np.exp(alpha_lambda_raw_[1])/(1+np.exp(alpha_lambda_raw_[1])), \n",
    "    def alpha_lambda_diff(alpha_lambda_raw_, control_pre):\n",
    "        ## Transform the inputted alpha,lambda values \n",
    "        alpha_lambda_t = alpha_lambda.alpha_lambda_transform(alpha_lambda_raw_)\n",
    "        difference_array = []\n",
    "        ## Pick one control unit as the pretend treatment unit\n",
    "        for u in control_pre.columns:\n",
    "            control_pre_placebo_treat = control_pre[u]\n",
    "            control_pre_placebo_cntrl = control_pre[ [l for l in control_pre.columns if l !=u] ]\n",
    "\n",
    "            ## Estimate mu and lambda with that control unit\n",
    "            control_pre_placebo_w = di.estimate_mu_omega(control_pre_placebo_treat,\n",
    "                             control_pre_placebo_cntrl,\n",
    "                             alpha_lambda_t)\n",
    "            ## Estimate the difference\n",
    "            d = alpha_lambda.diff(control_pre_placebo_treat, \n",
    "                 control_pre_placebo_cntrl, \n",
    "                 control_pre_placebo_w['mu'],\n",
    "                 control_pre_placebo_w['omega'])\n",
    "            difference_array.append(d)\n",
    "        ## Estimate the difference across all the control units\n",
    "        d_mean = np.mean(difference_array)\n",
    "        return d_mean\n",
    "    def get_alpha_lambda(control_pre_input):\n",
    "        ## Initialize at a given point\n",
    "        weights = minimize(partial(alpha_lambda.alpha_lambda_diff, control_pre=control_pre_input),\n",
    "                             np.array([10.15,0.5]),\n",
    "                           method='BFGS',\n",
    "                          options={'maxiter':5000, 'gtol': 1e-07, 'disp':False})\n",
    "        return weights\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "488a1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constrained Lasso Model\n",
    "from scipy.optimize import fmin_slsqp\n",
    "\n",
    "class cl:\n",
    "    def cl_obj(params, y,x) -> float:\n",
    "        return np.mean(  (y - params[0] - np.dot(x,params[1:]))**2)\n",
    "    \n",
    "    def predict_mu_omega(treatment_pre, control_pre, holdout_windows):\n",
    "        ## Don't use all the control data\n",
    "        ## Make sure that the holdout windows add up to the total number of pre-treatment units\n",
    "        if (holdout_windows[0]+holdout_windows[1] != len(control_pre)):\n",
    "            print('the arg holdout_windows does not add up to the number of time units!')\n",
    "            print('holdout_windows = {0}'.format(holdout_windows))\n",
    "            print('total number of time periods = {0}'.format(len(control_pre)))\n",
    "        else:\n",
    "            pass    \n",
    "        ## Define the holdout samples\n",
    "        control_holdout = control_pre[0:holdout_windows[0]].copy()\n",
    "        treatment_holdout = treatment_pre[0:holdout_windows[0]].copy()    \n",
    "        \n",
    "        control_nonholdout = control_pre[holdout_windows[0]:].copy()\n",
    "        treatment_nonholdout = treatment_pre[holdout_windows[0]:].copy()\n",
    "        \n",
    "        ## Estimate the CL model\n",
    "        ## Let's loop over different treatment units.\n",
    "        holdout_dict = {}\n",
    "        holdout_dict['mu'] = []\n",
    "        holdout_dict['omega'] = []\n",
    "        holdout_dict['weights'] = []\n",
    "        diff_holdout_mse = []\n",
    "        diff_nonholdout_mse = []\n",
    "        if treatment_pre.shape[1] > 1:\n",
    "            for t in treatment_pre.columns:\n",
    "                t_dict = cl.get_mu_omega(treatment_holdout[t], control_holdout)\n",
    "                ## Estimate measure of fit for the hold out and non-holdout sample\n",
    "                diff_h = treatment_holdout[t]       - np.dot(control_holdout, t_dict['omega'].T)+t_dict['mu']\n",
    "                diff_h_mse = (diff_h**2).mean()\n",
    "                diff_nh = treatment_nonholdout[t] - np.dot(control_nonholdout, t_dict['omega'].T)+t_dict['mu']\n",
    "                diff_nh_mse = (diff_nh**2).mean()\n",
    "        \n",
    "                holdout_dict['mu'].append(t_dict['mu'])\n",
    "                holdout_dict['omega'].append(t_dict['omega'])\n",
    "                holdout_dict['weights'].append(t_dict['weights'])\n",
    "                diff_holdout_mse.append(diff_h_mse)\n",
    "                diff_nonholdout_mse.append(diff_nh_mse)\n",
    "        else:\n",
    "            \n",
    "            t_dict = cl.get_mu_omega(treatment_holdout.values.flatten(), control_holdout)\n",
    "            ## Estimate measure of fit for the hold out and non-holdout sample\n",
    "            t_dict['omega'] = np.array([t_dict['omega']])\n",
    "            diff_h= treatment_holdout       - np.dot(control_holdout, t_dict['omega'].T)+t_dict['mu']\n",
    "            diff_h_mse = (diff_h**2).mean()\n",
    "            diff_nh = treatment_nonholdout - np.dot(control_nonholdout, t_dict['omega'].T)+t_dict['mu']\n",
    "            diff_nh_mse = (diff_nh**2).mean()\n",
    "\n",
    "            holdout_dict['mu'].append(t_dict['mu'])\n",
    "            holdout_dict['omega'].append(t_dict['omega'])\n",
    "            holdout_dict['weights'].append(t_dict['weights'])\n",
    "            diff_holdout_mse.append(diff_h_mse)\n",
    "            diff_nonholdout_mse.append(diff_nh_mse)            \n",
    "                    \n",
    "        holdout_dict['omega'] = np.array(holdout_dict['omega'])\n",
    "        if len(holdout_dict['omega'].shape) > 2:\n",
    "            holdout_dict['omega'] = holdout_dict['omega'].reshape(holdout_dict['omega'].shape[-2],holdout_dict['omega'].shape[-1])\n",
    "        else:\n",
    "            pass        \n",
    "        holdout_dict['mse_holdout'] =np.mean(diff_holdout_mse)\n",
    "        holdout_dict['mse_nonholdout'] =np.mean(diff_holdout_mse)        \n",
    "\n",
    "        return holdout_dict\n",
    "\n",
    "\n",
    "    def get_mu_omega(treatment_pre_input, control_pre_input):\n",
    "        n = control_pre_input.shape[1]\n",
    "        initialx = np.ones(n+1)/1\n",
    "        ## Initialize at a given point\n",
    "        weights = fmin_slsqp(partial(cl.cl_obj, y=treatment_pre_input,\n",
    "                                  x=control_pre_input),\n",
    "                             initialx,\n",
    "                             f_ieqcons=lambda x: 1-np.sum(np.abs(x[1:])),\n",
    "                         iter=50000, \n",
    "                         disp=False)\n",
    "        mu, omega = weights[0], weights[1:]\n",
    "        return {'mu':mu, 'omega':omega, 'weights':weights}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2d4eb46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Abadie, Diamond, Hainmueller (2010) model\n",
    "## Also code up the ADH weights\n",
    "## Abadie/Diamond/Hainmueller    \n",
    "from typing import List\n",
    "from operator import add\n",
    "from toolz import reduce, partial\n",
    "from scipy.optimize import fmin_slsqp\n",
    "\n",
    "class adh:\n",
    "    ## Define loss function\n",
    "    def loss_w(W, X, y) -> float:\n",
    "        return np.sqrt(np.mean((y - X.dot(W))**2))\n",
    "\n",
    "    def get_w(X, y):\n",
    "        ## Initialize at sample average with some noise\n",
    "        w_start = [1/X.shape[1]]*X.shape[1]\n",
    "    #     w_start = np.ones(X.shape[1])\n",
    "\n",
    "        weights = fmin_slsqp(partial(adh.loss_w, X=X, y=y),\n",
    "                             np.array(w_start),\n",
    "                             f_eqcons=lambda x: np.sum(x) - 1,\n",
    "                             iter=50000, \n",
    "                             bounds=[(0.0, 1.0)]*len(w_start),\n",
    "                             disp=False)\n",
    "        return weights   \n",
    "    \n",
    "    def predict_omega(treatment_pre, control_pre, holdout_windows):\n",
    "        ## Don't use all the control data\n",
    "        ## Make sure that the holdout windows add up to the total number of pre-treatment units\n",
    "        if (holdout_windows[0]+holdout_windows[1] != len(control_pre)):\n",
    "            print('the arg holdout_windows does not add up to the number of time units!')\n",
    "            print('holdout_windows = {0}'.format(holdout_windows))\n",
    "            print('total number of time periods = {0}'.format(len(control_pre)))\n",
    "        else:\n",
    "            pass    \n",
    "        ## Define the holdout samples\n",
    "        control_holdout = control_pre[0:holdout_windows[0]].copy()\n",
    "        treatment_holdout = treatment_pre[0:holdout_windows[0]].copy()    \n",
    "        \n",
    "        control_nonholdout = control_pre[holdout_windows[0]:].copy()\n",
    "        treatment_nonholdout = treatment_pre[holdout_windows[0]:].copy()\n",
    "        \n",
    "        ## Estimate the CL model\n",
    "        ## Let's loop over different treatment units.\n",
    "        holdout_dict = {}\n",
    "        holdout_dict['omega'] = []\n",
    "        holdout_dict['weights'] = []\n",
    "        diff_holdout_mse = []\n",
    "        diff_nonholdout_mse = []\n",
    "        if treatment_pre.shape[1] > 1:\n",
    "            for t in treatment_pre.columns:\n",
    "                t_dict = adh.get_w(control_holdout,treatment_holdout[t])\n",
    "                ## Estimate measure of fit for the hold out and non-holdout sample\n",
    "                diff_h = treatment_holdout[t]       - np.dot(control_holdout, t_dict.T)\n",
    "                diff_h_mse = (diff_h**2).mean()\n",
    "                diff_nh = treatment_nonholdout[t] - np.dot(control_nonholdout, t_dict.T)\n",
    "                diff_nh_mse = (diff_nh**2).mean()\n",
    "        \n",
    "                holdout_dict['omega'].append(t_dict)\n",
    "                holdout_dict['weights'].append(t_dict)\n",
    "                diff_holdout_mse.append(diff_h_mse)\n",
    "                diff_nonholdout_mse.append(diff_nh_mse)\n",
    "        else:\n",
    "            t_dict = adh.get_w(control_holdout,treatment_holdout.values.flatten())\n",
    "            ## Estimate measure of fit for the hold out and non-holdout sample\n",
    "            diff_h= treatment_holdout       - np.dot(control_holdout, np.array([t_dict]).T)\n",
    "            diff_h_mse = (diff_h**2).mean()\n",
    "            diff_nh = treatment_nonholdout - np.dot(control_nonholdout, np.array([t_dict]).T)\n",
    "            diff_nh_mse = (diff_nh**2).mean()\n",
    "\n",
    "            holdout_dict['omega'].append(t_dict)\n",
    "            holdout_dict['weights'].append(t_dict)\n",
    "            diff_holdout_mse.append(diff_h_mse)\n",
    "            diff_nonholdout_mse.append(diff_nh_mse)            \n",
    "        holdout_dict['omega'] = np.array(holdout_dict['omega'])\n",
    "        holdout_dict['mse_holdout'] =np.mean(diff_holdout_mse)\n",
    "        holdout_dict['mse_nonholdout'] =np.mean(diff_holdout_mse)        \n",
    "        \n",
    "        return holdout_dict    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5f16606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conformal Inference to do inference for the SC models\n",
    "import itertools\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng()\n",
    "rng.choice(10, size=10, replace=False)\n",
    "\n",
    "\n",
    "class conformal_inf:\n",
    "    def time_block_permutation(data=None,\n",
    "                              time_unit='date',\n",
    "                              post='W'):\n",
    "        tw =  len(data.loc[ data[post]==1][time_unit].unique())\n",
    "        treatment_window = len(data.loc[(data[post]==0)][time_unit].unique())-tw, tw\n",
    "\n",
    "        time_list = np.arange( np.sum(treatment_window) )\n",
    "        T_len = len(time_list)\n",
    "\n",
    "        ## Time block permutations\n",
    "        permutations_subset_block = []\n",
    "\n",
    "        for i in range(T_len):\n",
    "            half_A = time_list[-1*(T_len-i):]\n",
    "            half_B = time_list[0:i]\n",
    "            scrambled_list = np.concatenate([half_A, half_B]) \n",
    "            permutations_subset_block.append( list(scrambled_list)  )\n",
    "\n",
    "        return permutations_subset_block, treatment_window\n",
    "\n",
    "\n",
    "    def scrambled_residual(counterfactual, actual, \n",
    "                       scrambled_order,\n",
    "                      treatment_window):\n",
    "        '''\n",
    "        counterfactual   array of counterfactual estimates that are assumed to be ordered sequentially\n",
    "        actual           ``'' for actual values\n",
    "        scrambled_order  integer array that tells me how to scramble these\n",
    "        treatment_window list of two integers for the number of pre-treatment and post-treatment units\n",
    "        '''\n",
    "        counterfactual_ = counterfactual[scrambled_order].copy()        \n",
    "        actual_         = actual[scrambled_order].copy()\n",
    "        return np.abs(actual_ - counterfactual_)[ -1*treatment_window[1]:]\n",
    "    \n",
    "    def test_statS(q, treatment_window, residual_abs):\n",
    "        normed = np.sum(  np.power(residual_abs, q) )\n",
    "        return np.power( treatment_window[1]**(-0.5)*normed , 1/q)    \n",
    "\n",
    "    def pvalue_calc(counterfactual=None,\n",
    "                    actual=None, \n",
    "                    permutation_list=None,\n",
    "                   treatment_window=None,\n",
    "                   h0 = 0):\n",
    "#         print(treatment_window)\n",
    "        control_pst = counterfactual[-1*treatment_window[1]:].copy()\n",
    "        actual_pst  = actual[-1*treatment_window[1]:].copy()\n",
    "        actual_pst -= h0\n",
    "\n",
    "        ## Calculate the residual\n",
    "        residual_initial = np.abs(actual_pst - control_pst)         \n",
    "        S_q = conformal_inf.test_statS(1, treatment_window, residual_initial)\n",
    "#         print(residual_initial)\n",
    "        ## Now do a whole bunch of treatment time scrambles\n",
    "        ## We're going to permute over all time-based permutations\n",
    "        ## Adjust the actual by the null hypothesis \n",
    "        treat_ = actual.copy()\n",
    "        treat_[-1*treatment_window[1]:] -= h0\n",
    "        full_residual = np.abs(treat_ - counterfactual)\n",
    "        S_q_pi = []\n",
    "        for r,r_index in zip(permutation_list, range(len(permutation_list))):\n",
    "            scrambled_dates = np.array(list(r))              \n",
    "            residual_ = full_residual[scrambled_dates][-1*treatment_window[1]:].copy()\n",
    "#             if r_index==0:\n",
    "#                 print(residual_)            \n",
    "            S_q_pi.append(  conformal_inf.test_statS(1, treatment_window, residual_ )  )\n",
    "            \n",
    "        p_value = 1 - np.average( (np.array(S_q_pi) < S_q ) )\n",
    "        return p_value\n",
    "    \n",
    "    def ci_calc(y_hat=None,\n",
    "               y_act=None,\n",
    "               theta_grid=None,\n",
    "                permutation_list_ci = None,\n",
    "                treatment_window_ci =None,\n",
    "               alpha=0.05):\n",
    "        pv_grid = []\n",
    "        for t in theta_grid:\n",
    "#             print(t)\n",
    "            pv = conformal_inf.pvalue_calc(counterfactual=y_hat.copy(),\n",
    "                        actual=y_act.copy(), \n",
    "                        permutation_list = permutation_list_ci,\n",
    "                        treatment_window = treatment_window_ci,\n",
    "                        h0=t)\n",
    "            pv_grid.append(pv)   \n",
    "        ci_list = [ theta_grid[i] for i in range(len(pv_grid)) if pv_grid[i] > alpha ]\n",
    "#         print('\\nxxxxxxxx')\n",
    "#         for t, p in zip(theta_grid, pv_grid):\n",
    "#             print('{0:5.3f}  {1:5.3f}'.format(t,p))\n",
    "#         print([np.min(ci_list), np.max(ci_list)])\n",
    "        return {'theta_list':theta_grid, 'pvalue_list':pv_grid, 'ci_list':ci_list,\n",
    "               'ci_interval':[np.min(ci_list), np.max(ci_list)]}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83611126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b3391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3670939a",
   "metadata": {},
   "source": [
    "Test these functions out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a615d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "N = 50\n",
    "initial = np.random.uniform(0,2, N)\n",
    "df = pd.DataFrame(data={'y':initial,\n",
    "                  'unit_id':np.arange(N),\n",
    "                       'time':np.zeros(N).astype(int)})\n",
    "for t in range(10):\n",
    "    entry = df.iloc[-1*N:]\n",
    "    entry['y'] = entry['y']*0.80 + np.random.normal(0,1,N)*0.20\n",
    "    entry.loc[[True]*N,'time'] = t\n",
    "    df = pd.concat([df,entry])\n",
    "df['treated']        = df['unit_id'].isin([0,1])\n",
    "df['post'] = (df['time'] > 8)\n",
    "df['W'] =     df['treated']*    df['post']\n",
    "df['unit_id'] = df['unit_id'].apply(str)\n",
    "df.loc[df['W']==True, 'y'] += 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874b206",
   "metadata": {},
   "source": [
    "DiD Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "dd52ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hsujulia/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 18, but rank is 2\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "/Users/hsujulia/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 18, but rank is 2\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "/Users/hsujulia/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 18, but rank is 16\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "/Users/hsujulia/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 18, but rank is 16\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "source": [
    "did_twfe_results = did.twfe(data=df,\n",
    "                           data_dict={'treatment':'treated',\n",
    "                                     'date':'time',\n",
    "                                     'post':'post',\n",
    "                                     'unitid':'unit_id',\n",
    "                                     'outcome':'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b7d2090a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_id</th>\n",
       "      <th>time</th>\n",
       "      <th>y</th>\n",
       "      <th>y_hats</th>\n",
       "      <th>y_hat_counterfactual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.348564</td>\n",
       "      <td>1.499668</td>\n",
       "      <td>1.499668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082355</td>\n",
       "      <td>0.634414</td>\n",
       "      <td>0.634414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.260881</td>\n",
       "      <td>1.232793</td>\n",
       "      <td>1.232793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.081072</td>\n",
       "      <td>0.367539</td>\n",
       "      <td>0.367539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.339429</td>\n",
       "      <td>1.033755</td>\n",
       "      <td>1.033755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>0.168501</td>\n",
       "      <td>0.168501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.927677</td>\n",
       "      <td>0.881717</td>\n",
       "      <td>0.881717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.249233</td>\n",
       "      <td>0.016463</td>\n",
       "      <td>0.016463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.108435</td>\n",
       "      <td>0.739097</td>\n",
       "      <td>0.739097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.180454</td>\n",
       "      <td>-0.126157</td>\n",
       "      <td>-0.126157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.639602</td>\n",
       "      <td>0.633869</td>\n",
       "      <td>0.633869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.034449</td>\n",
       "      <td>-0.231385</td>\n",
       "      <td>-0.231385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.422187</td>\n",
       "      <td>0.537683</td>\n",
       "      <td>0.537683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.265259</td>\n",
       "      <td>-0.327571</td>\n",
       "      <td>-0.327571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.400371</td>\n",
       "      <td>0.509540</td>\n",
       "      <td>0.509540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.078283</td>\n",
       "      <td>-0.355714</td>\n",
       "      <td>-0.355714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.463010</td>\n",
       "      <td>0.463010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.226695</td>\n",
       "      <td>-0.402244</td>\n",
       "      <td>-0.402244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5.202085</td>\n",
       "      <td>5.034848</td>\n",
       "      <td>0.411947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4.764185</td>\n",
       "      <td>4.704931</td>\n",
       "      <td>-0.453307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4.867611</td>\n",
       "      <td>5.034848</td>\n",
       "      <td>0.411947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4.645677</td>\n",
       "      <td>4.704931</td>\n",
       "      <td>-0.453307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    unit_id  time         y    y_hats  y_hat_counterfactual\n",
       "0         0     0  1.348564  1.499668              1.499668\n",
       "1         1     0  0.082355  0.634414              0.634414\n",
       "50        0     1  1.260881  1.232793              1.232793\n",
       "51        1     1  0.081072  0.367539              0.367539\n",
       "100       0     2  1.339429  1.033755              1.033755\n",
       "101       1     2 -0.141193  0.168501              0.168501\n",
       "150       0     3  0.927677  0.881717              0.881717\n",
       "151       1     3 -0.249233  0.016463              0.016463\n",
       "200       0     4  1.108435  0.739097              0.739097\n",
       "201       1     4 -0.180454 -0.126157             -0.126157\n",
       "250       0     5  0.639602  0.633869              0.633869\n",
       "251       1     5  0.034449 -0.231385             -0.231385\n",
       "300       0     6  0.422187  0.537683              0.537683\n",
       "301       1     6  0.265259 -0.327571             -0.327571\n",
       "350       0     7  0.400371  0.509540              0.509540\n",
       "351       1     7  0.078283 -0.355714             -0.355714\n",
       "400       0     8  0.083984  0.463010              0.463010\n",
       "401       1     8 -0.226695 -0.402244             -0.402244\n",
       "450       0     9  5.202085  5.034848              0.411947\n",
       "451       1     9  4.764185  4.704931             -0.453307\n",
       "500       0     9  4.867611  5.034848              0.411947\n",
       "501       1     9  4.645677  4.704931             -0.453307"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_id</th>\n",
       "      <th>time</th>\n",
       "      <th>y</th>\n",
       "      <th>y_hats</th>\n",
       "      <th>y_hat_counterfactual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.348564</td>\n",
       "      <td>1.348564</td>\n",
       "      <td>1.348564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082355</td>\n",
       "      <td>0.082355</td>\n",
       "      <td>0.082355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.260881</td>\n",
       "      <td>1.260881</td>\n",
       "      <td>1.260881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.081072</td>\n",
       "      <td>0.081072</td>\n",
       "      <td>0.081072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.339429</td>\n",
       "      <td>1.339429</td>\n",
       "      <td>1.339429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>-0.141193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.927677</td>\n",
       "      <td>0.927677</td>\n",
       "      <td>0.927677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.249233</td>\n",
       "      <td>-0.249233</td>\n",
       "      <td>-0.249233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.108435</td>\n",
       "      <td>1.108435</td>\n",
       "      <td>1.108435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.180454</td>\n",
       "      <td>-0.180454</td>\n",
       "      <td>-0.180454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.639602</td>\n",
       "      <td>0.639602</td>\n",
       "      <td>0.639602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.034449</td>\n",
       "      <td>0.034449</td>\n",
       "      <td>0.034449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.422187</td>\n",
       "      <td>0.422187</td>\n",
       "      <td>0.422187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.265259</td>\n",
       "      <td>0.265259</td>\n",
       "      <td>0.265259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.400371</td>\n",
       "      <td>0.400371</td>\n",
       "      <td>0.400371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.078283</td>\n",
       "      <td>0.078283</td>\n",
       "      <td>0.078283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.455448</td>\n",
       "      <td>0.455448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.226695</td>\n",
       "      <td>0.455448</td>\n",
       "      <td>0.455448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5.202085</td>\n",
       "      <td>5.034848</td>\n",
       "      <td>0.455448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4.764185</td>\n",
       "      <td>4.704931</td>\n",
       "      <td>0.455448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4.867611</td>\n",
       "      <td>5.034848</td>\n",
       "      <td>0.455448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4.645677</td>\n",
       "      <td>4.704931</td>\n",
       "      <td>0.455448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    unit_id  time         y    y_hats  y_hat_counterfactual\n",
       "0         0     0  1.348564  1.348564              1.348564\n",
       "1         1     0  0.082355  0.082355              0.082355\n",
       "50        0     1  1.260881  1.260881              1.260881\n",
       "51        1     1  0.081072  0.081072              0.081072\n",
       "100       0     2  1.339429  1.339429              1.339429\n",
       "101       1     2 -0.141193 -0.141193             -0.141193\n",
       "150       0     3  0.927677  0.927677              0.927677\n",
       "151       1     3 -0.249233 -0.249233             -0.249233\n",
       "200       0     4  1.108435  1.108435              1.108435\n",
       "201       1     4 -0.180454 -0.180454             -0.180454\n",
       "250       0     5  0.639602  0.639602              0.639602\n",
       "251       1     5  0.034449  0.034449              0.034449\n",
       "300       0     6  0.422187  0.422187              0.422187\n",
       "301       1     6  0.265259  0.265259              0.265259\n",
       "350       0     7  0.400371  0.400371              0.400371\n",
       "351       1     7  0.078283  0.078283              0.078283\n",
       "400       0     8  0.083984  0.455448              0.455448\n",
       "401       1     8 -0.226695  0.455448              0.455448\n",
       "450       0     9  5.202085  5.034848              0.455448\n",
       "451       1     9  4.764185  4.704931              0.455448\n",
       "500       0     9  4.867611  5.034848              0.455448\n",
       "501       1     9  4.645677  4.704931              0.455448"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display( \n",
    "    did_twfe_results['twfe_c'].loc[did_twfe_results['twfe_c']['unit_id'].isin(['0','1'])]\n",
    ")\n",
    "display( \n",
    "    did_twfe_results['event_study_c'].loc[did_twfe_results['event_study_c']['unit_id'].isin(['0','1'])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6faa0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19de87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "048cd8af",
   "metadata": {},
   "source": [
    "SC Model Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc705b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean data\n",
    "sc_dict = dgp.clean_and_input_data(dataset=df,\n",
    "                                   treatment='treated',\n",
    "                                   unit_id='unit_id',\n",
    "                                   date='time',\n",
    "                                   post='post',\n",
    "                                  outcome='y')\n",
    "\n",
    "# ## Figure out the alpha and lambda values\n",
    "# w=alpha_lambda.get_alpha_lambda(sc_dict['C_pre'])\n",
    "# alpha_lambda_to_use = alpha_lambda.alpha_lambda_transform(w.x)\n",
    "# ## Take the alpha and lambda values, and estimate mu and omega\n",
    "# di_est = di.predict_mu_omega(sc_dict['T_pre'], sc_dict['C_pre'], alpha_lambda_to_use, \n",
    "#                              treatment_window_pre_treatment)\n",
    "# di_output = di.sc_style_results(sc_dict['T_pre'], sc_dict['T_pst'],\n",
    "#                     sc_dict['C_pre'], sc_dict['C_pst'],\n",
    "#                         di_est['mu'],di_est['omega'])\n",
    "# di_validation = sc.sc_validation(treatment_pre=sc_dict['T_pre'], \n",
    "#                  treatment_pst=sc_dict['T_pst'],\n",
    "#                  control_pre=sc_dict['C_pre'],\n",
    "#                  control_pst=sc_dict['C_pst'], \n",
    "#                  mu=di_est['mu'],\n",
    "#                  omega=di_est['omega'],\n",
    "#                  pre_treatment_window=treatment_window_pre_treatment)\n",
    "# cl_output['predict_est']\n",
    "\n",
    "# ak7 = cl.predict_mu_omega(sc_dict['T_pre'], sc_dict['C_pre'], treatment_window_pre_treatment)\n",
    "# cl_output = di.sc_style_results(sc_dict['T_pre'], sc_dict['T_pst'],\n",
    "#                     sc_dict['C_pre'], sc_dict['C_pst'],\n",
    "#                     ak7['mu'], ak7['omega'])\n",
    "# sc_validation = sc.sc_validation(treatment_pre=sc_dict['T_pre'], \n",
    "#                  treatment_pst=sc_dict['T_pst'],\n",
    "#                  control_pre=sc_dict['C_pre'],\n",
    "#                  control_pst=sc_dict['C_pst'], \n",
    "#                  mu=ak7['mu'],\n",
    "#                  omega=ak7['omega'],\n",
    "#                  pre_treatment_window=treatment_window_pre_treatment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = sc.sc_model(model_name='adh',\n",
    "        data=df,\n",
    "        data_dict={'treatment': 'treated',\n",
    "                  'date':'time',\n",
    "                  'post':'post',\n",
    "                  'unitid':'unit_id',\n",
    "                  'outcome':'y'},\n",
    "        pre_process_data=None,\n",
    "                 aggregate_pst_periods=True,\n",
    "        pre_treatment_window=None,\n",
    "        inference={'alpha':0.05, 'theta_grid':np.arange(-2,8,0.0151)})\n",
    "display( f3['results_df'] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f81d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.sc_generate_figures(final_sc_output=f3,\n",
    "                           output_figure_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e116c809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
